{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch02/01_main-chapter-code/the-verdict.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4P3NpEPsPiT",
        "outputId": "d927da72-6c4d-4e85-8c6d-102fd625c882"
      },
      "id": "X4P3NpEPsPiT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-24 13:27:32--  https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch02/01_main-chapter-code/the-verdict.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20479 (20K) [text/plain]\n",
            "Saving to: ‘the-verdict.txt’\n",
            "\n",
            "\rthe-verdict.txt       0%[                    ]       0  --.-KB/s               \rthe-verdict.txt     100%[===================>]  20.00K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2024-11-24 13:27:32 (22.3 MB/s) - ‘the-verdict.txt’ saved [20479/20479]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "455f4fc9-2d60-4de0-9177-dcb2abd5972a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "455f4fc9-2d60-4de0-9177-dcb2abd5972a",
        "outputId": "6958e82f-6fbe-4773-8de6-d3cc836582dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ],
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4dd3fc6-aa5e-4f09-aa99-97ac24689e96",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4dd3fc6-aa5e-4f09-aa99-97ac24689e96",
        "outputId": "2273b2a8-2f50-448f-edf0-34880e339671"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text = \"Hello, world. This, is a test.\"\n",
        "result = re.split(r'(\\s)', text)\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffe44724-a00a-40b9-8464-103b50028fe4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffe44724-a00a-40b9-8464-103b50028fe4",
        "outputId": "0431c12c-4166-466a-c333-81d7497c1b9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
          ]
        }
      ],
      "source": [
        "text = \"Hello, world. Is this-- a test?\"\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a18a4f6-18fc-4b4d-a7af-8e11ada501ab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a18a4f6-18fc-4b4d-a7af-8e11ada501ab",
        "outputId": "8e53cf6f-2a5b-4c85-b4ba-3a3129f33555"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
          ]
        }
      ],
      "source": [
        "text = \"Hello, world. Is this-- a test?\"\n",
        "\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71d3ce45-de9e-4d08-a6fb-c6c2aeb81a10",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71d3ce45-de9e-4d08-a6fb-c6c2aeb81a10",
        "outputId": "d9102c7c-d8ef-4421-a4c4-799c93982bb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
          ]
        }
      ],
      "source": [
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:30])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f752954-2f17-4579-9831-c98279849190",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f752954-2f17-4579-9831-c98279849190",
        "outputId": "d257ec35-e3bb-42f6-d5c1-6dd0d90c7bdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4690\n"
          ]
        }
      ],
      "source": [
        "print(len(preprocessed))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17033ba2-0879-486d-89b8-9265d7ad460b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17033ba2-0879-486d-89b8-9265d7ad460b",
        "outputId": "a8a09168-7ff8-436d-cbac-691880bcd14d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1130\n"
          ]
        }
      ],
      "source": [
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79dfa5a9-6f1f-4675-b738-7d4809b4a421",
      "metadata": {
        "id": "79dfa5a9-6f1f-4675-b738-7d4809b4a421"
      },
      "outputs": [],
      "source": [
        "vocab = {token:integer for integer,token in enumerate(all_words)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fc82880-3d69-4998-bb53-7632bf0e737a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fc82880-3d69-4998-bb53-7632bf0e737a",
        "outputId": "ad161eea-0a32-4316-fcc6-8e495294d061"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n",
            "('Burlington', 21)\n",
            "('But', 22)\n",
            "('By', 23)\n",
            "('Carlo', 24)\n",
            "('Chicago', 25)\n",
            "('Claude', 26)\n",
            "('Come', 27)\n",
            "('Croft', 28)\n",
            "('Destroyed', 29)\n",
            "('Devonshire', 30)\n",
            "('Don', 31)\n",
            "('Dubarry', 32)\n",
            "('Emperors', 33)\n",
            "('Florence', 34)\n",
            "('For', 35)\n",
            "('Gallery', 36)\n",
            "('Gideon', 37)\n",
            "('Gisburn', 38)\n",
            "('Gisburns', 39)\n",
            "('Grafton', 40)\n",
            "('Greek', 41)\n",
            "('Grindle', 42)\n",
            "('Grindles', 43)\n",
            "('HAD', 44)\n",
            "('Had', 45)\n",
            "('Hang', 46)\n",
            "('Has', 47)\n",
            "('He', 48)\n",
            "('Her', 49)\n",
            "('Hermia', 50)\n"
          ]
        }
      ],
      "source": [
        "for i, item in enumerate(vocab.items()):\n",
        "    print(item)\n",
        "    if i >= 50:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3285139f-4ba8-4e27-8179-43e1ab402196",
      "metadata": {
        "id": "3285139f-4ba8-4e27-8179-43e1ab402196"
      },
      "outputs": [],
      "source": [
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "        preprocessed = [\n",
        "            item.strip() for item in preprocessed if item.strip()\n",
        "        ]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a3ec2c6-d634-42f2-a80a-f01820c1e3b3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a3ec2c6-d634-42f2-a80a-f01820c1e3b3",
        "outputId": "c746b587-2a48-48b4-9663-5373bfa1942b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "text = \"\"\"\"It's the last he painted, you know,\"\n",
        "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cea09167-d1e5-4265-a335-5af29216b84e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cea09167-d1e5-4265-a335-5af29216b84e",
        "outputId": "f5743c65-318c-44dc-d7b2-e1f52aed822a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "tokenizer.decode(ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89360933-faea-4e75-a2a7-1d03cf5e5d6d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89360933-faea-4e75-a2a7-1d03cf5e5d6d",
        "outputId": "4f5ab08a-533a-4354-e26a-5d062b15cd17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n"
          ]
        }
      ],
      "source": [
        "! pip3 install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55e90a6f-88a3-4a18-b376-f0872135e219",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55e90a6f-88a3-4a18-b376-f0872135e219",
        "outputId": "bcf6c303-1c3d-42fe-a94a-359a8c8cbb42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.8.0\n"
          ]
        }
      ],
      "source": [
        "import importlib\n",
        "import tiktoken\n",
        "\n",
        "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e3089bd-32ac-455a-8629-88b45c473d56",
      "metadata": {
        "id": "2e3089bd-32ac-455a-8629-88b45c473d56"
      },
      "outputs": [],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2ead28b-b969-456b-bc84-ca711d8ede42",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2ead28b-b969-456b-bc84-ca711d8ede42",
        "outputId": "1598520c-21e9-443b-af6d-909ce3828bea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
          ]
        }
      ],
      "source": [
        "text = (\n",
        "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
        "     \"of someunknownPlace.\"\n",
        ")\n",
        "\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "print(integers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0cf7abf-899b-4d1c-897d-ba50a97a092b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0cf7abf-899b-4d1c-897d-ba50a97a092b",
        "outputId": "8262041e-0b8f-4808-b195-de3b2976e3b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
          ]
        }
      ],
      "source": [
        "strings = tokenizer.decode(integers)\n",
        "\n",
        "print(strings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70bec81c-8ad7-4a29-a010-a7bbbceca855",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70bec81c-8ad7-4a29-a010-a7bbbceca855",
        "outputId": "62ad3626-5722-4627-cefc-1b035e5d5d11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[33901, 86, 343, 86, 220, 959]\n",
            "Akwirw ier\n"
          ]
        }
      ],
      "source": [
        "integers = tokenizer.encode(\"Akwirw ier\")\n",
        "print(integers)\n",
        "\n",
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60d11b24-cd90-4d45-922d-0f4bcd3c41b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60d11b24-cd90-4d45-922d-0f4bcd3c41b5",
        "outputId": "63266024-8dbc-4543-904e-d7e2a7660dc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vocabulary size for GPT2 is: 50257\n",
            "The vocabulary size for GPT3 is: 50281\n",
            "The vocabulary size for GPT4 is: 100277\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "# Initialize the encodings for GPT-2, GPT-3, and GPT-4\n",
        "encodings = {\n",
        "    \"gpt2\": tiktoken.get_encoding(\"gpt2\"),\n",
        "    \"gpt3\": tiktoken.get_encoding(\"p50k_base\"),  # Commonly associated with GPT-3 models\n",
        "    \"gpt4\": tiktoken.get_encoding(\"cl100k_base\")  # Used for GPT-4 and later versions\n",
        "}\n",
        "\n",
        "# Get the vocabulary size for each encoding\n",
        "vocab_sizes = {model: encoding.n_vocab for model, encoding in encodings.items()}\n",
        "\n",
        "# Print the vocabulary sizes\n",
        "for model, size in vocab_sizes.items():\n",
        "    print(f\"The vocabulary size for {model.upper()} is: {size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12598d43-06a3-4bff-af21-a620b903f098",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12598d43-06a3-4bff-af21-a620b903f098",
        "outputId": "803fa7b9-93f8-48a9-d775-edb5e25466b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5145\n"
          ]
        }
      ],
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06498df8-899b-4807-98c8-5d8c0278f66b",
      "metadata": {
        "id": "06498df8-899b-4807-98c8-5d8c0278f66b"
      },
      "outputs": [],
      "source": [
        "enc_sample = enc_text[50:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad5dea71-62a8-4866-ad7f-29f5e85ee06a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad5dea71-62a8-4866-ad7f-29f5e85ee06a",
        "outputId": "7f79079d-4c3d-428c-90ff-f59bac26f10e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [290, 4920, 2241, 287]\n",
            "y:      [4920, 2241, 287, 257]\n"
          ]
        }
      ],
      "source": [
        "context_size = 4 #length of the input\n",
        "#The context_size of 4 means that the model is trained to look at a sequence of 4 words (or tokens)\n",
        "#to predict the next word in the sequence.\n",
        "#The input x is the first 4 tokens [1, 2, 3, 4], and the target y is the next 4 tokens [2, 3, 4, 5]\n",
        "\n",
        "x = enc_sample[:context_size]\n",
        "y = enc_sample[1:context_size+1]\n",
        "\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y:      {y}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8593076-f669-482d-9942-8efc2bd0a69a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8593076-f669-482d-9942-8efc2bd0a69a",
        "outputId": "c0fa81e5-f2ee-4277-e84a-a5021738642e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[290] ----> 4920\n",
            "[290, 4920] ----> 2241\n",
            "[290, 4920, 2241] ----> 287\n",
            "[290, 4920, 2241, 287] ----> 257\n"
          ]
        }
      ],
      "source": [
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "\n",
        "    print(context, \"---->\", desired)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27a99457-bba4-4741-b603-c51822c7bfb3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27a99457-bba4-4741-b603-c51822c7bfb3",
        "outputId": "1c313fd2-d512-4ad4-92fa-015191b969dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " and ---->  established\n",
            " and established ---->  himself\n",
            " and established himself ---->  in\n",
            " and established himself in ---->  a\n"
          ]
        }
      ],
      "source": [
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "\n",
        "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ffed6f2-3cb2-4b67-957c-c1a4cd4ef27f",
      "metadata": {
        "id": "8ffed6f2-3cb2-4b67-957c-c1a4cd4ef27f"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38fdc702-cc43-410c-b529-4d919163115a",
      "metadata": {
        "id": "38fdc702-cc43-410c-b529-4d919163115a"
      },
      "outputs": [],
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f506b28b-c1c0-41b0-96d5-b0443a9267ee",
      "metadata": {
        "id": "f506b28b-c1c0-41b0-96d5-b0443a9267ee"
      },
      "outputs": [],
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "829f808e-f331-4c12-b2f1-36ce0b5af30f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "829f808e-f331-4c12-b2f1-36ce0b5af30f",
        "outputId": "99ae4669-2df0-4459-c944-869a1d5f5961"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.5.1+cu121\n",
            "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b8d0e52-b760-4865-9bd3-088353a41042",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b8d0e52-b760-4865-9bd3-088353a41042",
        "outputId": "7cda6eea-1960-4f82-8374-24d0abbbf6d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
          ]
        }
      ],
      "source": [
        "second_batch = next(data_iter)\n",
        "print(second_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "895b5de7-c8d1-4027-9f8f-47e535ad9e94",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "895b5de7-c8d1-4027-9f8f-47e535ad9e94",
        "outputId": "54dcd113-5758-498a-c7fd-959aa84a884f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Targets:\n",
            " tensor([[  367,  2885,  1464,  1807],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [ 2138,   257,  7026, 15632],\n",
            "        [  438,  2016,   257,   922],\n",
            "        [ 5891,  1576,   438,   568],\n",
            "        [  340,   373,   645,  1049],\n",
            "        [ 5975,   284,   502,   284],\n",
            "        [ 3285,   326,    11,   287]])\n"
          ]
        }
      ],
      "source": [
        "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a753bb9-d149-4313-9e54-6742f9b776d8",
      "metadata": {
        "id": "3a753bb9-d149-4313-9e54-6742f9b776d8"
      },
      "outputs": [],
      "source": [
        "input_ids = torch.tensor([2, 3, 5, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d616a3ab-a3f3-4917-932c-0873645676a2",
      "metadata": {
        "id": "d616a3ab-a3f3-4917-932c-0873645676a2"
      },
      "outputs": [],
      "source": [
        "vocab_size = 6\n",
        "output_dim = 3\n",
        "\n",
        "torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42324830-28eb-479a-8c9a-cd205e2df583",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42324830-28eb-479a-8c9a-cd205e2df583",
        "outputId": "fe4cc236-e2c8-4818-89a8-2aece96eeb09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.1690],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-1.1589,  0.3255, -0.6315],\n",
            "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "print(embedding_layer.weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd2be17e-e7f6-44a0-95fe-fd1d5f21bb2e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd2be17e-e7f6-44a0-95fe-fd1d5f21bb2e",
        "outputId": "3bb19d59-d627-4248-8510-42d299f6e1b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-2.8400, -0.7849, -1.4096],\n",
            "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(embedding_layer(input_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b618885-d5a6-4205-9c01-53416bbea75d",
      "metadata": {
        "id": "9b618885-d5a6-4205-9c01-53416bbea75d"
      },
      "outputs": [],
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7344af0d-59ca-41b3-9e01-fc16a213409a",
      "metadata": {
        "id": "7344af0d-59ca-41b3-9e01-fc16a213409a"
      },
      "outputs": [],
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=max_length,\n",
        "    stride=max_length, shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0994e480-a622-41ff-82e8-85ca53d4609e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0994e480-a622-41ff-82e8-85ca53d4609e",
        "outputId": "2d1cd153-c9f7-45d4-9294-7728c0175395"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n"
          ]
        }
      ],
      "source": [
        "print(\"Token IDs:\\n\", inputs)\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bf14fe0-50b2-474e-a99b-6f061dd1feea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bf14fe0-50b2-474e-a99b-6f061dd1feea",
        "outputId": "4686e09c-598f-4b63-c736-f946f6887cfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ],
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a742716-f8f6-4a81-a7d4-77f96bab4076",
      "metadata": {
        "id": "3a742716-f8f6-4a81-a7d4-77f96bab4076"
      },
      "outputs": [],
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ad3368f-5bd1-42d5-83e5-4c0624773acb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ad3368f-5bd1-42d5-83e5-4c0624773acb",
        "outputId": "b6882bf6-522d-4704-cd40-97ceed98e8e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ],
      "source": [
        "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a6428aa-dd3e-4f54-89de-0f80a545147c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a6428aa-dd3e-4f54-89de-0f80a545147c",
        "outputId": "23408e25-f42f-4f31-d1d5-c947a6c48633"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ],
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(input_embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a9226f5-39bf-46c3-8012-590f7a67e1fb",
      "metadata": {
        "id": "4a9226f5-39bf-46c3-8012-590f7a67e1fb"
      },
      "outputs": [],
      "source": [
        "## Simplified Attention mechanism"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a85ccdb-cfdb-46bf-b4a3-66a86b50a1e9",
      "metadata": {
        "id": "0a85ccdb-cfdb-46bf-b4a3-66a86b50a1e9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c587fb31-c58b-4d8e-a348-8aaa1c88efd3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c587fb31-c58b-4d8e-a348-8aaa1c88efd3",
        "outputId": "8ddb10ab-6f67-4fcd-d512-dbf041d0af20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
          ]
        }
      ],
      "source": [
        "query = inputs[1]  # 2nd input token is the query\n",
        "\n",
        "attn_scores_2 = torch.empty(inputs.shape[0])\n",
        "for i, x_i in enumerate(inputs):\n",
        "    attn_scores_2[i] = torch.dot(x_i, query) # dot product (transpose not necessary here since they are 1-dim vectors)\n",
        "\n",
        "print(attn_scores_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8676fa79-e8e9-4578-b63e-1d9f21448354",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8676fa79-e8e9-4578-b63e-1d9f21448354",
        "outputId": "c0c29b3a-b0fb-470c-a83d-7aa2ab0726b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
            "Sum: tensor(1.0000)\n"
          ]
        }
      ],
      "source": [
        "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
        "\n",
        "print(\"Attention weights:\", attn_weights_2_tmp)\n",
        "print(\"Sum:\", attn_weights_2_tmp.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa516e3a-b940-43fe-ac9a-d83b5068f45a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa516e3a-b940-43fe-ac9a-d83b5068f45a",
        "outputId": "922bf659-0b1c-4a32-8b20-1a70bbf244da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "Sum: tensor(1.)\n"
          ]
        }
      ],
      "source": [
        "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
        "print(\"Attention weights:\", attn_weights_2)\n",
        "print(\"Sum:\", attn_weights_2.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c303afb-dc97-409e-a702-bb47dfe181a2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c303afb-dc97-409e-a702-bb47dfe181a2",
        "outputId": "ed4d934d-b6c4-4b90-a3f4-c8c077c892be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4419, 0.6515, 0.5683])\n"
          ]
        }
      ],
      "source": [
        "query = inputs[1] # 2nd input token is the query\n",
        "\n",
        "context_vec_2 = torch.zeros(query.shape)\n",
        "for i,x_i in enumerate(inputs):\n",
        "    context_vec_2 += attn_weights_2[i]*x_i\n",
        "\n",
        "print(context_vec_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "697de719-3ccc-495c-9e1d-4988542f5ec6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "697de719-3ccc-495c-9e1d-4988542f5ec6",
        "outputId": "dd046e41-4bf6-4c32-e93c-ec219964c50a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ],
      "source": [
        "attn_scores = torch.empty(6, 6)\n",
        "\n",
        "for i, x_i in enumerate(inputs):\n",
        "    for j, x_j in enumerate(inputs):\n",
        "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
        "\n",
        "print(attn_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4f32b04-7af3-47a4-886c-f7e2315f0a09",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4f32b04-7af3-47a4-886c-f7e2315f0a09",
        "outputId": "234bdfcd-e95b-41d7-aa15-c30663a0b8d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ],
      "source": [
        "attn_scores = inputs @ inputs.T\n",
        "print(attn_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "549e5142-2673-4190-be76-33dce165accb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "549e5142-2673-4190-be76-33dce165accb",
        "outputId": "1869512e-b848-44aa-f8c5-45958babfc94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
            "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
            "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
            "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
            "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
            "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
          ]
        }
      ],
      "source": [
        "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "print(attn_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95167c1b-a017-48be-972d-3815e0672d08",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95167c1b-a017-48be-972d-3815e0672d08",
        "outputId": "cfc70ca4-dab3-48c1-f9d5-6750f30a2e6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 2 sum: 1.0\n",
            "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
          ]
        }
      ],
      "source": [
        "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
        "print(\"Row 2 sum:\", row_2_sum)\n",
        "print(\"All row sums:\", attn_weights.sum(dim=-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "902ac242-93b9-40e2-916f-5e4a63e96223",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "902ac242-93b9-40e2-916f-5e4a63e96223",
        "outputId": "416fdaa3-82a4-4d31-d688-28287031645e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4421, 0.5931, 0.5790],\n",
            "        [0.4419, 0.6515, 0.5683],\n",
            "        [0.4431, 0.6496, 0.5671],\n",
            "        [0.4304, 0.6298, 0.5510],\n",
            "        [0.4671, 0.5910, 0.5266],\n",
            "        [0.4177, 0.6503, 0.5645]])\n"
          ]
        }
      ],
      "source": [
        "all_context_vecs = attn_weights @ inputs\n",
        "print(all_context_vecs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0073902-c665-468c-8c6f-190db4226956",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0073902-c665-468c-8c6f-190db4226956",
        "outputId": "a396e4ea-5360-4152-e59a-95ca5b370800"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Previous 2nd context vector: tensor([0.4419, 0.6515, 0.5683])\n"
          ]
        }
      ],
      "source": [
        "print(\"Previous 2nd context vector:\", context_vec_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e87a3ad2-817b-4475-bc27-a965759d7568",
      "metadata": {
        "id": "e87a3ad2-817b-4475-bc27-a965759d7568"
      },
      "outputs": [],
      "source": [
        "## IMPLEMENTING SELF ATTENTION WITH TRAINABLE WEIGHTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09e1aedc-5654-4086-8ba0-587ce67467eb",
      "metadata": {
        "id": "09e1aedc-5654-4086-8ba0-587ce67467eb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "242bba3a-8531-4bb5-9951-a99330c00836",
      "metadata": {
        "id": "242bba3a-8531-4bb5-9951-a99330c00836"
      },
      "outputs": [],
      "source": [
        "x_2 = inputs[1] #A\n",
        "d_in = inputs.shape[1] #B\n",
        "d_out = 2 #C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de771f4f-e75b-4395-a3bf-0b7e69825aab",
      "metadata": {
        "id": "de771f4f-e75b-4395-a3bf-0b7e69825aab"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e76d163-7b3b-4813-bfe1-40988cdadcc0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e76d163-7b3b-4813-bfe1-40988cdadcc0",
        "outputId": "9fb36ed7-c2ea-46ef-a9ad-6abc750962e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[0.2961, 0.5166],\n",
            "        [0.2517, 0.6886],\n",
            "        [0.0740, 0.8665]])\n"
          ]
        }
      ],
      "source": [
        "print(W_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5308b19c-c1b8-45fa-8f89-ca45d7c84404",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5308b19c-c1b8-45fa-8f89-ca45d7c84404",
        "outputId": "5b5438e8-8f7b-4d80-ad5f-abc884526b75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4306, 1.4551])\n"
          ]
        }
      ],
      "source": [
        "query_2 = x_2 @ W_query\n",
        "key_2 = x_2 @ W_key\n",
        "value_2 = x_2 @ W_value\n",
        "print(query_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "116101f1-9d23-4ff4-8d75-d73c400c2871",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "116101f1-9d23-4ff4-8d75-d73c400c2871",
        "outputId": "0e626045-8e80-4e1a-cc5c-5d7b7096982c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keys.shape: torch.Size([6, 2])\n",
            "values.shape: torch.Size([6, 2])\n",
            "queries.shape: torch.Size([6, 2])\n"
          ]
        }
      ],
      "source": [
        "keys = inputs @ W_key\n",
        "values = inputs @ W_value\n",
        "queries = inputs @ W_query\n",
        "print(\"keys.shape:\", keys.shape)\n",
        "\n",
        "print(\"values.shape:\", values.shape)\n",
        "\n",
        "print(\"queries.shape:\", queries.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72f1c255-3848-4ec3-8a00-3e9235f47292",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72f1c255-3848-4ec3-8a00-3e9235f47292",
        "outputId": "ef240413-bc84-4b33-df3f-48d910ec2b37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.8524)\n"
          ]
        }
      ],
      "source": [
        "keys_2 = keys[1] #A\n",
        "attn_score_22 = query_2.dot(keys_2)\n",
        "print(attn_score_22)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3484e3f3-e8f3-454c-a233-74199e574d10",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3484e3f3-e8f3-454c-a233-74199e574d10",
        "outputId": "a2ea2360-4ded-4784-bec8-f366a124199b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
          ]
        }
      ],
      "source": [
        "attn_scores_2 = query_2 @ keys.T # All attention scores for given query\n",
        "print(attn_scores_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33e5ab3c-7a6e-468a-9cb6-b624602df889",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33e5ab3c-7a6e-468a-9cb6-b624602df889",
        "outputId": "15083466-3e57-49e1-eb08-c443c1e90d20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9231, 1.3545, 1.3241, 0.7910, 0.4032, 1.1330],\n",
            "        [1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
            "        [1.2544, 1.8284, 1.7877, 1.0654, 0.5508, 1.5238],\n",
            "        [0.6973, 1.0167, 0.9941, 0.5925, 0.3061, 0.8475],\n",
            "        [0.6114, 0.8819, 0.8626, 0.5121, 0.2707, 0.7307],\n",
            "        [0.8995, 1.3165, 1.2871, 0.7682, 0.3937, 1.0996]])\n"
          ]
        }
      ],
      "source": [
        "attn_scores = queries @ keys.T # omega\n",
        "print(attn_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b438c01-314e-4165-8241-f0cbd5558659",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b438c01-314e-4165-8241-f0cbd5558659",
        "outputId": "506c0c86-7ad0-4d39-e2c2-6b1e41cbee1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "d_k = keys.shape[-1]\n",
        "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
        "print(attn_weights_2)\n",
        "print(d_k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a5560b8-982c-410e-adcd-5018f2143268",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a5560b8-982c-410e-adcd-5018f2143268",
        "outputId": "adc618de-892a-439c-cc88-19ca4d341f78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Softmax without scaling: tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n",
            "Softmax after scaling (tensor * 8): tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Define the tensor\n",
        "tensor = torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])\n",
        "\n",
        "# Apply softmax without scaling\n",
        "softmax_result = torch.softmax(tensor, dim=-1)\n",
        "print(\"Softmax without scaling:\", softmax_result)\n",
        "\n",
        "# Multiply the tensor by 8 and then apply softmax\n",
        "scaled_tensor = tensor * 8\n",
        "softmax_scaled_result = torch.softmax(scaled_tensor, dim=-1)\n",
        "print(\"Softmax after scaling (tensor * 8):\", softmax_scaled_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8699a11b-9bb0-4971-be06-4a160903bc1f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8699a11b-9bb0-4971-be06-4a160903bc1f",
        "outputId": "682bb5ad-dfb2-4fed-ab54-85d461c18028"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.3061, 0.8210])\n"
          ]
        }
      ],
      "source": [
        "context_vec_2 = attn_weights_2 @ values\n",
        "print(context_vec_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1879007-6766-4c22-9272-d1c07ab96b45",
      "metadata": {
        "id": "b1879007-6766-4c22-9272-d1c07ab96b45"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention_v1(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out):\n",
        "        super().__init__()\n",
        "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
        "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
        "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = x @ self.W_key\n",
        "        queries = x @ self.W_query\n",
        "        values = x @ self.W_value\n",
        "\n",
        "        attn_scores = queries @ keys.T # omega\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
        "        )\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac7161cd-ec89-46a9-9271-f7165a8b3898",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac7161cd-ec89-46a9-9271-f7165a8b3898",
        "outputId": "b875ea4c-fccc-4831-8c33-da19a25ddd94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2996, 0.8053],\n",
            "        [0.3061, 0.8210],\n",
            "        [0.3058, 0.8203],\n",
            "        [0.2948, 0.7939],\n",
            "        [0.2927, 0.7891],\n",
            "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
        "print(sa_v1(inputs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae19b0f1-2816-4a04-9dec-f99b7607d0ff",
      "metadata": {
        "id": "ae19b0f1-2816-4a04-9dec-f99b7607d0ff"
      },
      "outputs": [],
      "source": [
        "class SelfAttention_v2(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.T\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2edf8866-92fb-4d05-ac7e-b200c6aac49d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2edf8866-92fb-4d05-ac7e-b200c6aac49d",
        "outputId": "dbd4ec71-482e-4884-b161-3e30286e529d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0739,  0.0713],\n",
            "        [-0.0748,  0.0703],\n",
            "        [-0.0749,  0.0702],\n",
            "        [-0.0760,  0.0685],\n",
            "        [-0.0763,  0.0679],\n",
            "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(789)\n",
        "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
        "print(sa_v2(inputs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7f32bda-2095-4967-9a42-10c371b360dc",
      "metadata": {
        "id": "b7f32bda-2095-4967-9a42-10c371b360dc"
      },
      "outputs": [],
      "source": [
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "735432ec-3909-49c0-bad2-057e43d32943",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "735432ec-3909-49c0-bad2-057e43d32943",
        "outputId": "1c0bb4c1-168e-437c-892f-825cc33b890f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
            "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
            "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
            "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
            "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "queries = sa_v2.W_query(inputs) #A\n",
        "keys = sa_v2.W_key(inputs)\n",
        "attn_scores = queries @ keys.T\n",
        "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n",
        "print(attn_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "218c04df-269b-464c-a695-4e8cca09144c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "218c04df-269b-464c-a695-4e8cca09144c",
        "outputId": "ecd374a6-d596-4a03-f210-6cfbe34b87ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "torch.ones(context_length, context_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75dc406b-f098-4c22-8ce3-a3d5de1ceffe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75dc406b-f098-4c22-8ce3-a3d5de1ceffe",
        "outputId": "b8ea5e9b-aea1-40c3-fb8a-12de57d180b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "context_length = attn_scores.shape[0]\n",
        "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
        "print(mask_simple)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7036af03-800e-45bf-93bf-144b69fbe340",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7036af03-800e-45bf-93bf-144b69fbe340",
        "outputId": "891d2bcd-d430-4b8e-cbc7-46ab82504a3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
            "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "masked_simple = attn_weights*mask_simple\n",
        "print(masked_simple)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1686eca1-f118-4431-9f9a-272ae3f39455",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1686eca1-f118-4431-9f9a-272ae3f39455",
        "outputId": "7bc8b026-9bac-48bd-8bd6-b3a07faf3805"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
            "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<DivBackward0>)\n"
          ]
        }
      ],
      "source": [
        "row_sums = masked_simple.sum(dim=1, keepdim=True)\n",
        "masked_simple_norm = masked_simple / row_sums\n",
        "print(masked_simple_norm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cef94b69-d95d-42bc-96ad-b5e518ab08c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cef94b69-d95d-42bc-96ad-b5e518ab08c4",
        "outputId": "ee2cfc2e-c314-48c9-b662-2a98a68c1de1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.2899,  0.0716,  0.0760, -0.0138,  0.1344, -0.0511],\n",
            "        [ 0.4656,  0.1723,  0.1751,  0.0259,  0.1771,  0.0085],\n",
            "        [ 0.4594,  0.1703,  0.1731,  0.0259,  0.1745,  0.0090],\n",
            "        [ 0.2642,  0.1024,  0.1036,  0.0186,  0.0973,  0.0122],\n",
            "        [ 0.2183,  0.0874,  0.0882,  0.0177,  0.0786,  0.0144],\n",
            "        [ 0.3408,  0.1270,  0.1290,  0.0198,  0.1290,  0.0078]],\n",
            "       grad_fn=<MmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(attn_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1018d0f-99e8-41ed-ad18-86ac7cfb255c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1018d0f-99e8-41ed-ad18-86ac7cfb255c",
        "outputId": "1135e491-13dc-4dfa-bfc7-01725b0c538d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1., 1., 1.],\n",
              "        [0., 1., 1., 1., 1., 1.],\n",
              "        [0., 0., 1., 1., 1., 1.],\n",
              "        [0., 0., 0., 1., 1., 1.],\n",
              "        [0., 0., 0., 0., 1., 1.],\n",
              "        [0., 0., 0., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "torch.triu(torch.ones(context_length, context_length))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46d5fb44-1a75-48fd-ba42-8f1eaeb4ef9e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46d5fb44-1a75-48fd-ba42-8f1eaeb4ef9e",
        "outputId": "936491f2-54de-40ad-d85d-a95f869e6e38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 1., 1., 1., 1., 1.],\n",
            "        [0., 0., 1., 1., 1., 1.],\n",
            "        [0., 0., 0., 1., 1., 1.],\n",
            "        [0., 0., 0., 0., 1., 1.],\n",
            "        [0., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "print(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2bbe754-10c4-49b0-a808-717d46c6d8b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2bbe754-10c4-49b0-a808-717d46c6d8b9",
        "outputId": "519f23c5-e802-4b69-b8bd-71013a9823d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
            "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
            "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
            "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n"
          ]
        }
      ],
      "source": [
        "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
        "print(masked)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7a81c16-978d-458e-b1dd-bedce20e9fda",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7a81c16-978d-458e-b1dd-bedce20e9fda",
        "outputId": "3ee325db-0565-4ff2-b5e3-f0c425823371"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
            "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
        "print(attn_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5e0c011-de43-4ecb-82bd-7f2655a47bc1",
      "metadata": {
        "id": "b5e0c011-de43-4ecb-82bd-7f2655a47bc1"
      },
      "outputs": [],
      "source": [
        "##IMPLEMENTING A COMPACT CAUSAL ATTENTION CLASS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fe24f71-b560-427a-ae2c-e969a675f98e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fe24f71-b560-427a-ae2c-e969a675f98e",
        "outputId": "7e13707e-2cc8-456c-d4d9-c8806af4379e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 6, 3])\n"
          ]
        }
      ],
      "source": [
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")\n",
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "print(batch.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5a84a26-230d-415d-a17d-0689f4cdf5db",
      "metadata": {
        "id": "a5a84a26-230d-415d-a17d-0689f4cdf5db"
      },
      "outputs": [],
      "source": [
        "class CausalAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, context_length,\n",
        "                 dropout, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.d_out = d_out\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.dropout = nn.Dropout(dropout) # New\n",
        "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape # New batch dimension b\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n",
        "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
        "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
        "        attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
        "        )\n",
        "        attn_weights = self.dropout(attn_weights) # New\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e06a3e56-fe44-43ac-ae5d-8ece6bbf1c07",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e06a3e56-fe44-43ac-ae5d-8ece6bbf1c07",
        "outputId": "246be68e-2d6a-496c-cf0c-c24a048122fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ],
      "source": [
        "print(d_in)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bbebc3c-5e2d-4ee0-a5de-7f4e2c0c5370",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bbebc3c-5e2d-4ee0-a5de-7f4e2c0c5370",
        "outputId": "682bacec-14f5-49af-8561-eea2a9316f49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ],
      "source": [
        "print(d_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08eb6f6a-ee0e-46af-b0d5-d08c8e1ae409",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08eb6f6a-ee0e-46af-b0d5-d08c8e1ae409",
        "outputId": "2579ec92-6a43-4f03-c8ec-6d9cd8c6320a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context_vecs.shape: torch.Size([2, 6, 2])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "context_length = batch.shape[1]\n",
        "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
        "context_vecs = ca(batch)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c053e057-5c33-43ba-a3be-6ad762955b56",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c053e057-5c33-43ba-a3be-6ad762955b56",
        "outputId": "22a351e1-f31e-4a82-ae27-ab7d8baf686d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.4519,  0.2216],\n",
            "         [-0.5874,  0.0058],\n",
            "         [-0.6300, -0.0632],\n",
            "         [-0.5675, -0.0843],\n",
            "         [-0.5526, -0.0981],\n",
            "         [-0.5299, -0.1081]],\n",
            "\n",
            "        [[-0.4519,  0.2216],\n",
            "         [-0.5874,  0.0058],\n",
            "         [-0.6300, -0.0632],\n",
            "         [-0.5675, -0.0843],\n",
            "         [-0.5526, -0.0981],\n",
            "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(context_vecs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fed428a-eef2-42e8-bb2c-2d4bbe27e59f",
      "metadata": {
        "id": "7fed428a-eef2-42e8-bb2c-2d4bbe27e59f"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(\n",
        "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n",
        "             for _ in range(num_heads)]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([head(x) for head in self.heads], dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0b240f8-f9a8-4893-ac9c-552015a3a29e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0b240f8-f9a8-4893-ac9c-552015a3a29e",
        "outputId": "6f6f724a-8de4-4866-a4d7-c490cba8e3b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 6, 3])\n"
          ]
        }
      ],
      "source": [
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")\n",
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "print(batch.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c9cf250-2bd6-475d-a9e9-bdb7360302d6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c9cf250-2bd6-475d-a9e9-bdb7360302d6",
        "outputId": "d4e3f7be-f2ca-4beb-94da-309af4b8189d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
            "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
            "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
            "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
            "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
            "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
            "\n",
            "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
            "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
            "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
            "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
            "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
            "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 6, 4])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "context_length = batch.shape[1] # This is the number of tokens = 6\n",
        "d_in, d_out = 3, 2\n",
        "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
        "context_vecs = mha(batch)\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b38cc8e9-3571-4d83-8d6c-d873a5e7babc",
      "metadata": {
        "id": "b38cc8e9-3571-4d83-8d6c-d873a5e7babc"
      },
      "outputs": [],
      "source": [
        "## IMPLEMENTING MULTI-HEAD ATTENTION WITH WEIGHT SPLITS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af61955a-0510-4a2a-b936-111e3b2f2aa5",
      "metadata": {
        "id": "af61955a-0510-4a2a-b936-111e3b2f2aa5"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36be8b34-a9fb-4644-b336-93ad922d5c9f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36be8b34-a9fb-4644-b336-93ad922d5c9f",
        "outputId": "9b737bbb-d627-4094-bf56-66ddb971d5ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 6])\n",
            "tensor([[[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
            "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
            "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]],\n",
            "\n",
            "        [[ 0.1569, -0.0873,  0.0210,  0.0215, -0.3243, -0.2518],\n",
            "         [ 0.1117, -0.0547,  0.0406, -0.0213, -0.3251, -0.2993],\n",
            "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 3, 6])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "# Define the tensor with 3 rows and 6 columns\n",
        "inputs = torch.tensor(\n",
        "    [[0.43, 0.15, 0.89, 0.55, 0.87, 0.66],  # Row 1\n",
        "     [0.57, 0.85, 0.64, 0.22, 0.58, 0.33],  # Row 2\n",
        "     [0.77, 0.25, 0.10, 0.05, 0.80, 0.55]]  # Row 3\n",
        ")\n",
        "\n",
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "print(batch.shape)\n",
        "\n",
        "batch_size, context_length, d_in = batch.shape\n",
        "d_out = 6\n",
        "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
        "context_vecs = mha(batch)\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "060dd86e-63f5-48e6-871e-87a16cc907ba",
      "metadata": {
        "id": "060dd86e-63f5-48e6-871e-87a16cc907ba"
      },
      "outputs": [],
      "source": [
        "## IMPLEMENTING A GPT MODEL FROM SCRATCH TO GENERATE TEXT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b674653-584d-4c97-86d1-91a87b133cf1",
      "metadata": {
        "id": "2b674653-584d-4c97-86d1-91a87b133cf1"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 1024, # Context length\n",
        "    \"emb_dim\": 768,         # Embedding dimension\n",
        "    \"n_heads\": 12,          # Number of attention heads\n",
        "    \"n_layers\": 12,         # Number of layers\n",
        "    \"drop_rate\": 0.1,       # Dropout rate\n",
        "    \"qkv_bias\": False       # Query-Key-Value bias\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "322b91ca-f080-4711-8b89-4ed798fb2c66",
      "metadata": {
        "id": "322b91ca-f080-4711-8b89-4ed798fb2c66"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class DummyGPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        # Use a placeholder for TransformerBlock\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        # Use a placeholder for LayerNorm\n",
        "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "class DummyTransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        # A simple placeholder\n",
        "\n",
        "    def forward(self, x):\n",
        "        # This block does nothing and just returns its input.\n",
        "        return x\n",
        "\n",
        "\n",
        "class DummyLayerNorm(nn.Module):\n",
        "    def __init__(self, normalized_shape, eps=1e-5):\n",
        "        super().__init__()\n",
        "        # The parameters here are just to mimic the LayerNorm interface.\n",
        "\n",
        "    def forward(self, x):\n",
        "        # This layer does nothing and just returns its input.\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3e1f09d-c64b-4fbd-96d9-1816f2a9a32d",
      "metadata": {
        "id": "e3e1f09d-c64b-4fbd-96d9-1816f2a9a32d"
      },
      "outputs": [],
      "source": [
        "## STEP 1: TOKENIZATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1a2be70-ccbe-4d4b-b8f2-ec04efaeb124",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1a2be70-ccbe-4d4b-b8f2-ec04efaeb124",
        "outputId": "3b65971c-739c-40c3-e712-b0f40f5f1bec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "batch = []\n",
        "txt1 = \"Every effort moves you\"\n",
        "txt2 = \"Every day holds a\"\n",
        "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "batch = torch.stack(batch, dim=0)\n",
        "print(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3987d648-e766-4e28-8eed-e9a398c8e7b5",
      "metadata": {
        "id": "3987d648-e766-4e28-8eed-e9a398c8e7b5"
      },
      "outputs": [],
      "source": [
        "## STEP 2: CREATE AN INSTANCE OF DUMMYGPTMODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b48023c-3f85-487a-81a6-9c9e61b88f15",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b48023c-3f85-487a-81a6-9c9e61b88f15",
        "outputId": "d3f16db4-023f-4dc3-92d6-36d5ff31002d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([2, 4, 50257])\n",
            "tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n",
            "         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],\n",
            "         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n",
            "         [ 0.0447,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n",
            "\n",
            "        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n",
            "         [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],\n",
            "         [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],\n",
            "         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "model = DummyGPTModel(GPT_CONFIG_124M)\n",
        "logits = model(batch)\n",
        "print(\"Output shape:\", logits.shape)\n",
        "print(logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41816846-bbbc-47f1-bad7-8617c1a2bb46",
      "metadata": {
        "id": "41816846-bbbc-47f1-bad7-8617c1a2bb46"
      },
      "outputs": [],
      "source": [
        "##GPT ARCHITECTURE PART 2: LAYER NORMALIZATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "753a6503-70fc-4f93-a25f-cc7f368dfdf3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "753a6503-70fc-4f93-a25f-cc7f368dfdf3",
        "outputId": "4c3565d1-398c-4914-d3ca-a3bbdad9e704"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
            "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
            "       grad_fn=<ReluBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "batch_example = torch.randn(2, 5) #A\n",
        "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
        "out = layer(batch_example)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c8c8060-fa18-4af4-8880-0c6f2a705d99",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c8c8060-fa18-4af4-8880-0c6f2a705d99",
        "outputId": "478d13b4-e0d9-449b-cc36-a694a704f001"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean:\n",
            " tensor([[0.1324],\n",
            "        [0.2170]], grad_fn=<MeanBackward1>)\n",
            "Variance:\n",
            " tensor([[0.0231],\n",
            "        [0.0398]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ],
      "source": [
        "mean = out.mean(dim=-1, keepdim=True)\n",
        "var = out.var(dim=-1, keepdim=True)\n",
        "print(\"Mean:\\n\", mean)\n",
        "print(\"Variance:\\n\", var)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90a665be-d3c1-49cc-8803-72ea22e0e7e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90a665be-d3c1-49cc-8803-72ea22e0e7e2",
        "outputId": "aabc92e7-6ea9-4e90-f2fd-6bdecab07b60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized layer outputs:\n",
            " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
            "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
            "       grad_fn=<DivBackward0>)\n",
            "Mean:\n",
            " tensor([[9.9341e-09],\n",
            "        [1.9868e-08]], grad_fn=<MeanBackward1>)\n",
            "Variance:\n",
            " tensor([[1.0000],\n",
            "        [1.0000]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ],
      "source": [
        "out_norm = (out - mean) / torch.sqrt(var)\n",
        "mean = out_norm.mean(dim=-1, keepdim=True)\n",
        "var = out_norm.var(dim=-1, keepdim=True)\n",
        "print(\"Normalized layer outputs:\\n\", out_norm)\n",
        "print(\"Mean:\\n\", mean)\n",
        "print(\"Variance:\\n\", var)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ea90ebc-ffdb-4208-ac60-4182fd780300",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ea90ebc-ffdb-4208-ac60-4182fd780300",
        "outputId": "19dfa58f-32c6-4795-eb30-0552951f9879"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean:\n",
            " tensor([[    0.0000],\n",
            "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
            "Variance:\n",
            " tensor([[1.0000],\n",
            "        [1.0000]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.set_printoptions(sci_mode=False)\n",
        "print(\"Mean:\\n\", mean)\n",
        "print(\"Variance:\\n\", var)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e734ae00-eec9-4fa4-a44a-aaa0225b3457",
      "metadata": {
        "id": "e734ae00-eec9-4fa4-a44a-aaa0225b3457"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed542caf-1b24-47e9-bdd2-13d4f4eb70cf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed542caf-1b24-47e9-bdd2-13d4f4eb70cf",
        "outputId": "34abd9b7-fdb2-4d96-8f01-02ddd97418cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean:\n",
            " tensor([[    -0.0000],\n",
            "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
            "Variance:\n",
            " tensor([[1.0000],\n",
            "        [1.0000]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ],
      "source": [
        "ln = LayerNorm(emb_dim=5)\n",
        "out_ln = ln(batch_example)\n",
        "mean = out_ln.mean(dim=-1, keepdim=True)\n",
        "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
        "print(\"Mean:\\n\", mean)\n",
        "print(\"Variance:\\n\", var)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "305d884d-76d9-4c85-8d22-3a43f0809acc",
      "metadata": {
        "id": "305d884d-76d9-4c85-8d22-3a43f0809acc"
      },
      "outputs": [],
      "source": [
        "## GPT ARCHITECTURE PART 3: FEEDFORWARD NEURAL NETWORK WITH GELU ACTIVATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c489637-6f74-45b1-8a2a-0eec4b14305d",
      "metadata": {
        "id": "1c489637-6f74-45b1-8a2a-0eec4b14305d"
      },
      "outputs": [],
      "source": [
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af1b98f7-1681-4a0b-953d-9ef969c7a87b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "af1b98f7-1681-4a0b-953d-9ef969c7a87b",
        "outputId": "f51f5872-6719-4a9c-ec77-200874ffe739"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "gelu, relu = GELU(), nn.ReLU()\n",
        "\n",
        "# Some sample data\n",
        "x = torch.linspace(-3, 3, 100)\n",
        "y_gelu, y_relu = gelu(x), relu(x)\n",
        "\n",
        "plt.figure(figsize=(8, 3))\n",
        "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
        "    plt.subplot(1, 2, i)\n",
        "    plt.plot(x, y)\n",
        "    plt.title(f\"{label} activation function\")\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(f\"{label}(x)\")\n",
        "    plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7c2032d-eedd-46f2-86be-c36f6a38173e",
      "metadata": {
        "id": "e7c2032d-eedd-46f2-86be-c36f6a38173e"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ef3f6ba-5a83-48d1-89f8-c19974e97b32",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ef3f6ba-5a83-48d1-89f8-c19974e97b32",
        "outputId": "d749163d-0fe3-4b5f-fd20-6e2d299db2ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "768\n"
          ]
        }
      ],
      "source": [
        "print(GPT_CONFIG_124M[\"emb_dim\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e3c8273-c7bf-43f8-926b-8441ebd59671",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e3c8273-c7bf-43f8-926b-8441ebd59671",
        "outputId": "8a2d9a74-3ac2-48e8-862e-1b1b5581355f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 768])\n"
          ]
        }
      ],
      "source": [
        "ffn = FeedForward(GPT_CONFIG_124M)\n",
        "x = torch.rand(2, 3, 768) #A\n",
        "out = ffn(x)\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0812d00b-ee70-4e45-9105-0fa5a93bd4ed",
      "metadata": {
        "id": "0812d00b-ee70-4e45-9105-0fa5a93bd4ed"
      },
      "outputs": [],
      "source": [
        "## GPT ARCHITECTURE PART 4: SHORTCUT CONNECTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f050ac29-aa95-4344-8ef8-1e73f33c56c3",
      "metadata": {
        "id": "f050ac29-aa95-4344-8ef8-1e73f33c56c3"
      },
      "outputs": [],
      "source": [
        "class ExampleDeepNeuralNetwork(nn.Module):\n",
        "    def __init__(self, layer_sizes, use_shortcut):\n",
        "        super().__init__()\n",
        "        self.use_shortcut = use_shortcut\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
        "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
        "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
        "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
        "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            # Compute the output of the current layer\n",
        "            layer_output = layer(x)\n",
        "            # Check if shortcut can be applied\n",
        "            if self.use_shortcut and x.shape == layer_output.shape:\n",
        "                x = x + layer_output\n",
        "            else:\n",
        "                x = layer_output\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63857679-4873-43ee-bcdf-36adce0cf986",
      "metadata": {
        "id": "63857679-4873-43ee-bcdf-36adce0cf986"
      },
      "outputs": [],
      "source": [
        "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
        "sample_input = torch.tensor([[1., 0., -1.]])\n",
        "torch.manual_seed(123) # specify random seed for the initial weights for reproducibility\n",
        "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
        "layer_sizes, use_shortcut=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7165628c-2b33-45c2-acb1-87896c0a270b",
      "metadata": {
        "id": "7165628c-2b33-45c2-acb1-87896c0a270b"
      },
      "outputs": [],
      "source": [
        "def print_gradients(model, x):\n",
        "    # Forward pass\n",
        "    output = model(x)\n",
        "    target = torch.tensor([[0.]])\n",
        "\n",
        "    # Calculate loss based on how close the target\n",
        "    # and output are\n",
        "    loss = nn.MSELoss()\n",
        "    loss = loss(output, target)\n",
        "\n",
        "    # Backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            # Print the mean absolute gradient of the weights\n",
        "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d63da2d-98db-4cd2-99c1-16ef2b6fee26",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d63da2d-98db-4cd2-99c1-16ef2b6fee26",
        "outputId": "31b900aa-c603-473d-e21f-f34065442705"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layers.0.0.weight has gradient mean of 0.00020173584925942123\n",
            "layers.1.0.weight has gradient mean of 0.00012011159560643137\n",
            "layers.2.0.weight has gradient mean of 0.0007152040489017963\n",
            "layers.3.0.weight has gradient mean of 0.0013988736318424344\n",
            "layers.4.0.weight has gradient mean of 0.005049645435065031\n"
          ]
        }
      ],
      "source": [
        "print_gradients(model_without_shortcut, sample_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87c76272-8e9b-4798-afc7-6c6890f981ae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87c76272-8e9b-4798-afc7-6c6890f981ae",
        "outputId": "a530ad59-06f0-4cd6-e876-f6c3e577c736"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layers.0.0.weight has gradient mean of 0.22169791162014008\n",
            "layers.1.0.weight has gradient mean of 0.20694105327129364\n",
            "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
            "layers.3.0.weight has gradient mean of 0.2665732204914093\n",
            "layers.4.0.weight has gradient mean of 1.3258540630340576\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
        "layer_sizes, use_shortcut=True\n",
        ")\n",
        "print_gradients(model_with_shortcut, sample_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da47caa7-ee15-4b4e-aea8-22cd5f4f7140",
      "metadata": {
        "id": "da47caa7-ee15-4b4e-aea8-22cd5f4f7140"
      },
      "outputs": [],
      "source": [
        "## GPT ARCHITECTURE PART 5: CODING ATTENTION AND LINEAR LAYERS IN A TRANSFORMER BLOCK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6313dc1-8b77-4988-bb0a-83167539339f",
      "metadata": {
        "id": "d6313dc1-8b77-4988-bb0a-83167539339f"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0d8450e-4497-498d-a077-bdefc633b385",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0d8450e-4497-498d-a077-bdefc633b385",
        "outputId": "413e5133-9a27-4a60-93a1-3fdc2ec755ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([2, 4, 768])\n",
            "Output shape: torch.Size([2, 4, 768])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "x = torch.rand(2, 4, 768) #A\n",
        "block = TransformerBlock(GPT_CONFIG_124M)\n",
        "output = block(x)\n",
        "print(\"Input shape:\", x.shape)\n",
        "print(\"Output shape:\", output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95566d28-8c05-4d5a-8b29-ca426e02bb2d",
      "metadata": {
        "id": "95566d28-8c05-4d5a-8b29-ca426e02bb2d"
      },
      "outputs": [],
      "source": [
        "## GPT ARCHITECTURE PART 6: ENTIRE GPT MODEL ARCHITECTURE IMPLEMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "909d4e47-e5e0-42d2-b355-f0c3916dfc2b",
      "metadata": {
        "id": "909d4e47-e5e0-42d2-b355-f0c3916dfc2b"
      },
      "outputs": [],
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8df41a00-db14-4064-8364-336c76ebc553",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8df41a00-db14-4064-8364-336c76ebc553",
        "outputId": "93bc33e9-6fad-44f6-96f0-5bb00c25f159"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch:\n",
            " tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n",
            "\n",
            "Output shape: torch.Size([2, 4, 50257])\n",
            "tensor([[[ 0.1381,  0.0077, -0.1963,  ..., -0.0222, -0.1060,  0.1717],\n",
            "         [ 0.3865, -0.8408, -0.6564,  ..., -0.5163,  0.2369, -0.3357],\n",
            "         [ 0.6989, -0.1829, -0.1631,  ...,  0.1472, -0.6504, -0.0056],\n",
            "         [-0.4290,  0.1669, -0.1258,  ...,  1.1579,  0.5303, -0.5549]],\n",
            "\n",
            "        [[ 0.1094, -0.2894, -0.1467,  ..., -0.0557,  0.2911, -0.2824],\n",
            "         [ 0.0882, -0.3552, -0.3527,  ...,  1.2930,  0.0053,  0.1898],\n",
            "         [ 0.6091,  0.4702, -0.4094,  ...,  0.7688,  0.3787, -0.1974],\n",
            "         [-0.0612, -0.0737,  0.4751,  ...,  1.2463, -0.3834,  0.0609]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "out = model(batch)\n",
        "print(\"Input batch:\\n\", batch)\n",
        "print(\"\\nOutput shape:\", out.shape)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dfa8967-51f8-4d7a-bfef-1dce2f9ba8b2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dfa8967-51f8-4d7a-bfef-1dce2f9ba8b2",
        "outputId": "4aefe695-27b0-42bb-87d8-8f500f8dd0e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 163,009,536\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59a5fca4-275d-4c1a-b3b5-d90b8ca17149",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59a5fca4-275d-4c1a-b3b5-d90b8ca17149",
        "outputId": "de477d77-1a31-40f4-a5d3-4e18bd5d84ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token embedding layer shape: torch.Size([50257, 768])\n",
            "Output layer shape: torch.Size([50257, 768])\n"
          ]
        }
      ],
      "source": [
        "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
        "print(\"Output layer shape:\", model.out_head.weight.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01476056-e9e4-4a4c-a19e-30c4def16a00",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01476056-e9e4-4a4c-a19e-30c4def16a00",
        "outputId": "24faeb4e-50e2-4d83-c369-e546e743311b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trainable parameters considering weight tying: 124,412,160\n"
          ]
        }
      ],
      "source": [
        "total_params_gpt2 = total_params - sum(p.numel() for p in model.out_head.parameters())\n",
        "print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5c49f90-82e7-40ac-9bfe-dfea8f4d5ba2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5c49f90-82e7-40ac-9bfe-dfea8f4d5ba2",
        "outputId": "01eac7ab-17fe-4bc0-ab6f-5094dca8780d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total size of the model: 621.83 MB\n"
          ]
        }
      ],
      "source": [
        "total_size_bytes = total_params * 4 #A\n",
        "total_size_mb = total_size_bytes / (1024 * 1024) #B\n",
        "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9860d0e9-e1ce-4c53-abe9-7b2d43d0111c",
      "metadata": {
        "id": "9860d0e9-e1ce-4c53-abe9-7b2d43d0111c"
      },
      "outputs": [],
      "source": [
        "## GPT ARCHITECTURE PART 7: GENERATING TEXT FROM OUTPUT TOKENS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89680630-2b27-4d79-96fb-24c29ee83921",
      "metadata": {
        "id": "89680630-2b27-4d79-96fb-24c29ee83921"
      },
      "outputs": [],
      "source": [
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # idx is (batch, n_tokens) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        # Crop current context if it exceeds the supported context size\n",
        "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
        "        # then only the last 5 tokens are used as context\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # Get the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        # Focus only on the last time step\n",
        "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Apply softmax to get probabilities\n",
        "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
        "\n",
        "        # Get the idx of the vocab entry with the highest probability value\n",
        "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "        # Append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c1961ed-2167-4cb2-8800-80188dd85b4f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c1961ed-2167-4cb2-8800-80188dd85b4f",
        "outputId": "f53b54ef-a551-455e-a6b9-7f85fa40be9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoded: [15496, 11, 314, 716]\n",
            "encoded_tensor.shape: torch.Size([1, 4])\n"
          ]
        }
      ],
      "source": [
        "start_context = \"Hello, I am\"\n",
        "encoded = tokenizer.encode(start_context)\n",
        "print(\"encoded:\", encoded)\n",
        "encoded_tensor = torch.tensor(encoded).unsqueeze(0) #A\n",
        "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d509ed7a-4acc-46c2-bac9-b03a750ffdbc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d509ed7a-4acc-46c2-bac9-b03a750ffdbc",
        "outputId": "dd9330bc-96c6-4f85-97a4-5677bb128432"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
            "Output length: 10\n"
          ]
        }
      ],
      "source": [
        "model.eval() #A\n",
        "out = generate_text_simple(\n",
        "model=model,\n",
        "idx=encoded_tensor,\n",
        "max_new_tokens=6,\n",
        "context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "print(\"Output:\", out)\n",
        "print(\"Output length:\", len(out[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdaa0f57-bc6f-4f2b-acc6-c5789aa0b634",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdaa0f57-bc6f-4f2b-acc6-c5789aa0b634",
        "outputId": "4fada553-a5b8-4c9c-ea07-6303ca52d7c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, I am Featureiman Byeswickattribute argue\n"
          ]
        }
      ],
      "source": [
        "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
        "print(decoded_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b73ca60-0657-4a87-871f-e888f670cf75",
      "metadata": {
        "id": "7b73ca60-0657-4a87-871f-e888f670cf75"
      },
      "outputs": [],
      "source": [
        "## EVALUATING GENERATIVE TEXT MODELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2c6cf8f-f0a4-4ec1-b8e7-cc7fa81c9949",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2c6cf8f-f0a4-4ec1-b8e7-cc7fa81c9949",
        "outputId": "42967959-2885-468b-ddd9-28e021c95726"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Every effort moves you Aeiman Byeswickattributeometer inspector Normandy freezerigrate\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "start_context = \"Every effort moves you\"\n",
        "\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(start_context, tokenizer),\n",
        "    max_new_tokens=10,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41ca848a-dc03-4536-970b-344d7fcd9134",
      "metadata": {
        "id": "41ca848a-dc03-4536-970b-344d7fcd9134"
      },
      "outputs": [],
      "source": [
        "## Calculating the text generation loss: cross-entropy and perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0842d268-42fb-4bb9-b4f6-f5b7ae678fe4",
      "metadata": {
        "id": "0842d268-42fb-4bb9-b4f6-f5b7ae678fe4"
      },
      "outputs": [],
      "source": [
        "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
        "                       [40,    1107, 588]])   #  \"I really like\"]\n",
        "\n",
        "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
        "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6b85ba1-8bff-41a0-8967-8553e35d8169",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6b85ba1-8bff-41a0-8967-8553e35d8169",
        "outputId": "8d9a5613-5cae-43f0-b6ff-2171677ff4fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 50257])\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    logits = model(inputs)\n",
        "\n",
        "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
        "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f497689-629f-4381-ba84-4e59525747ae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f497689-629f-4381-ba84-4e59525747ae",
        "outputId": "2945dba2-8c96-43c0-959e-a196b34f834e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[[36397],\n",
            "         [39619],\n",
            "         [20610]],\n",
            "\n",
            "        [[ 8615],\n",
            "         [49289],\n",
            "         [47105]]])\n"
          ]
        }
      ],
      "source": [
        "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
        "print(\"Token IDs:\\n\", token_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf9aa439-3015-4f4f-ade2-5ece06284a84",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf9aa439-3015-4f4f-ade2-5ece06284a84",
        "outputId": "673b388b-5d29-43ec-e1ec-b1d7ceaf5d2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Targets batch 1:  effort moves you\n",
            "Outputs batch 1:  Gathering SerbianFriday\n"
          ]
        }
      ],
      "source": [
        "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
        "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21e477f5-1f91-4557-81e0-c9f4e1180c8e",
      "metadata": {
        "id": "21e477f5-1f91-4557-81e0-c9f4e1180c8e"
      },
      "outputs": [],
      "source": [
        "## Cross-entropy loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fb149c5-2b09-46fe-8ef8-716d76a10fc5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fb149c5-2b09-46fe-8ef8-716d76a10fc5",
        "outputId": "fef93696-020a-4c92-c579-b9eaae44302d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text 1: tensor([    0.0000,     0.0000,     0.0000])\n",
            "Text 2: tensor([    0.0000,     0.0000,     0.0000])\n"
          ]
        }
      ],
      "source": [
        "text_idx = 0\n",
        "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
        "print(\"Text 1:\", target_probas_1)\n",
        "\n",
        "text_idx = 1\n",
        "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
        "print(\"Text 2:\", target_probas_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6f06f2a-0a93-412c-b88c-dccfeb7aa6ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6f06f2a-0a93-412c-b88c-dccfeb7aa6ca",
        "outputId": "2b6882a0-9eca-44a4-9002-1c76a114b3cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-10.6600, -10.7936, -11.3531, -10.0591, -11.0276, -11.3658])\n"
          ]
        }
      ],
      "source": [
        "# Compute logarithm of all token probabilities\n",
        "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
        "print(log_probas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6acc8557-89e0-4c4f-91f8-54961d21ae50",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6acc8557-89e0-4c4f-91f8-54961d21ae50",
        "outputId": "3eb1a29d-1b0a-401d-9ab1-058c34c08397"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-10.8765)\n"
          ]
        }
      ],
      "source": [
        "# Calculate the average probability for each token\n",
        "avg_log_probas = torch.mean(log_probas)\n",
        "print(avg_log_probas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d422fda3-5a1d-4798-a9aa-6b85646c85ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d422fda3-5a1d-4798-a9aa-6b85646c85ea",
        "outputId": "3578ac40-9fcf-43c3-c90d-062ed981f414"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(10.8765)\n"
          ]
        }
      ],
      "source": [
        "neg_avg_log_probas = avg_log_probas * -1\n",
        "print(neg_avg_log_probas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0134f383-f820-4a69-b2c2-af9b2747d133",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0134f383-f820-4a69-b2c2-af9b2747d133",
        "outputId": "a8e7d3d2-99a8-4ee9-e7f4-e7f130588641"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits shape: torch.Size([2, 3, 50257])\n",
            "Targets shape: torch.Size([2, 3])\n"
          ]
        }
      ],
      "source": [
        "# Logits have shape (batch_size, num_tokens, vocab_size)\n",
        "print(\"Logits shape:\", logits.shape)\n",
        "\n",
        "# Targets have shape (batch_size, num_tokens)\n",
        "print(\"Targets shape:\", targets.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa73726a-6597-49e0-93f7-dd97c5623363",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa73726a-6597-49e0-93f7-dd97c5623363",
        "outputId": "4b664788-3e3b-4a40-8f5a-a592d8ed86cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flattened logits: torch.Size([6, 50257])\n",
            "Flattened targets: torch.Size([6])\n"
          ]
        }
      ],
      "source": [
        "logits_flat = logits.flatten(0, 1)\n",
        "targets_flat = targets.flatten()\n",
        "\n",
        "print(\"Flattened logits:\", logits_flat.shape)\n",
        "print(\"Flattened targets:\", targets_flat.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d5b9e91-3082-47d8-b92e-5ae7d5fdf4f0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d5b9e91-3082-47d8-b92e-5ae7d5fdf4f0",
        "outputId": "803f0fc4-19ba-44d1-816d-334e30614b40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(10.8765)\n"
          ]
        }
      ],
      "source": [
        "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ef86176-a675-4514-beb6-cc4bfc343bd4",
      "metadata": {
        "id": "0ef86176-a675-4514-beb6-cc4bfc343bd4"
      },
      "outputs": [],
      "source": [
        "## Perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "792e34b6-33b3-4fcd-b72e-7230fc632cb5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "792e34b6-33b3-4fcd-b72e-7230fc632cb5",
        "outputId": "3e9f3f43-f297-49f8-b647-2f012faa26b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(52918.7773)\n"
          ]
        }
      ],
      "source": [
        "perplexity = torch.exp(loss)\n",
        "print(perplexity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f86c4424-787c-4781-8fd3-d9dd3a125fad",
      "metadata": {
        "id": "f86c4424-787c-4781-8fd3-d9dd3a125fad"
      },
      "outputs": [],
      "source": [
        "## Calculating the training and validation set losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c79d2ac-7098-4e71-83dd-dce0b1901ff8",
      "metadata": {
        "id": "1c79d2ac-7098-4e71-83dd-dce0b1901ff8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "\n",
        "file_path = \"the-verdict.txt\"\n",
        "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    with urllib.request.urlopen(url) as response:\n",
        "        text_data = response.read().decode('utf-8')\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(text_data)\n",
        "else:\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        text_data = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e04b72cc-eda7-47d2-a306-0140e82e6bfa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e04b72cc-eda7-47d2-a306-0140e82e6bfa",
        "outputId": "8cf18653-0dac-4352-8208-8e370365dc43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ],
      "source": [
        "# First 100 characters\n",
        "print(text_data[:99])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66e64a30-8368-4ba6-b070-ee1d4ab32edf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66e64a30-8368-4ba6-b070-ee1d4ab32edf",
        "outputId": "9c881680-a568-41a6-f2a7-81a8ae659a52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art.\"\n"
          ]
        }
      ],
      "source": [
        "# Last 100 characters\n",
        "print(text_data[-99:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51ed61ea-bcd3-4049-980e-5d299806d87f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51ed61ea-bcd3-4049-980e-5d299806d87f",
        "outputId": "2b464e1e-a16b-4260-f3e1-d1c8c11d0e32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Characters: 20479\n",
            "Tokens: 5145\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "total_characters = len(text_data)\n",
        "total_tokens = len(tokenizer.encode(text_data))\n",
        "\n",
        "print(\"Characters:\", total_characters)\n",
        "print(\"Tokens:\", total_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74291f4b-fc23-4452-bc3d-8a5163795fe5",
      "metadata": {
        "id": "74291f4b-fc23-4452-bc3d-8a5163795fe5"
      },
      "outputs": [],
      "source": [
        "## Implementing the DataLoader:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01bef886-c3c7-4136-92f7-d2ed3f3b7030",
      "metadata": {
        "id": "01bef886-c3c7-4136-92f7-d2ed3f3b7030"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6e8a5a4-1c18-4751-9c8b-5fa1a8a20350",
      "metadata": {
        "id": "d6e8a5a4-1c18-4751-9c8b-5fa1a8a20350"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,   # Vocabulary size\n",
        "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
        "    \"emb_dim\": 768,        # Embedding dimension\n",
        "    \"n_heads\": 12,         # Number of attention heads\n",
        "    \"n_layers\": 12,        # Number of layers\n",
        "    \"drop_rate\": 0.1,      # Dropout rate\n",
        "    \"qkv_bias\": False      # Query-key-value bias\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2ba3af1-afb1-4ab1-baa5-7d09fdaba256",
      "metadata": {
        "id": "e2ba3af1-afb1-4ab1-baa5-7d09fdaba256"
      },
      "outputs": [],
      "source": [
        "# Train/validation ratio\n",
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio * len(text_data))\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = create_dataloader_v1(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = create_dataloader_v1(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4d002bc-5139-464a-9ab5-d45ac5a8b2b3",
      "metadata": {
        "id": "a4d002bc-5139-464a-9ab5-d45ac5a8b2b3"
      },
      "outputs": [],
      "source": [
        "# Sanity check\n",
        "\n",
        "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
        "    print(\"Not enough tokens for the training loader. \"\n",
        "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
        "          \"increase the `training_ratio`\")\n",
        "\n",
        "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
        "    print(\"Not enough tokens for the validation loader. \"\n",
        "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
        "          \"decrease the `training_ratio`\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81c899e7-ff30-42ea-b265-f278cad945e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81c899e7-ff30-42ea-b265-f278cad945e7",
        "outputId": "faeb4ec3-1cf6-458a-b66d-7ac098f98e66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loader:\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "\n",
            "Validation loader:\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "9\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "print(\"Train loader:\")\n",
        "for x, y in train_loader:\n",
        "    print(x.shape, y.shape)\n",
        "\n",
        "print(\"\\nValidation loader:\")\n",
        "for x, y in val_loader:\n",
        "    print(x.shape, y.shape)\n",
        "\n",
        "print(len(train_loader))\n",
        "print(len(val_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34addfdf-a685-48ba-9b9d-f90f7f2774ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34addfdf-a685-48ba-9b9d-f90f7f2774ce",
        "outputId": "0ed2ea98-d433-4589-a4b7-e00806e2b4f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training tokens: 4608\n",
            "Validation tokens: 512\n",
            "All tokens: 5120\n"
          ]
        }
      ],
      "source": [
        "train_tokens = 0\n",
        "for input_batch, target_batch in train_loader:\n",
        "    train_tokens += input_batch.numel()\n",
        "\n",
        "val_tokens = 0\n",
        "for input_batch, target_batch in val_loader:\n",
        "    val_tokens += input_batch.numel()\n",
        "\n",
        "print(\"Training tokens:\", train_tokens)\n",
        "print(\"Validation tokens:\", val_tokens)\n",
        "print(\"All tokens:\", train_tokens + val_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4da95fd-eca6-4383-a6ba-00a93aad39c0",
      "metadata": {
        "id": "e4da95fd-eca6-4383-a6ba-00a93aad39c0"
      },
      "outputs": [],
      "source": [
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.eval();  # Disable dropout during inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2270575-c58c-4254-8abb-91fc0cbbba63",
      "metadata": {
        "id": "e2270575-c58c-4254-8abb-91fc0cbbba63"
      },
      "outputs": [],
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0d6801b-414e-4ec9-98d9-f4331aecc1a1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0d6801b-414e-4ec9-98d9-f4331aecc1a1",
        "outputId": "c133b814-9452-4030-b80b-bb6539090ab6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device.\n",
            "Training loss: 10.98758347829183\n",
            "Validation loss: 10.98110580444336\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Note:\n",
        "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
        "# which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
        "# However, the resulting loss values may be slightly different.\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"Using {device} device.\")\n",
        "\n",
        "\n",
        "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
        "\n",
        "\n",
        "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
        "\n",
        "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
        "    train_loss = calc_loss_loader(train_loader, model, device)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device)\n",
        "\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86468a46-7ed6-43cf-9db7-bb5897d6b375",
      "metadata": {
        "id": "86468a46-7ed6-43cf-9db7-bb5897d6b375"
      },
      "outputs": [],
      "source": [
        "# TRAINING LOOP FOR THE LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f849f1de-05af-42bf-aafb-c58021816d66",
      "metadata": {
        "id": "f849f1de-05af-42bf-aafb-c58021816d66"
      },
      "outputs": [],
      "source": [
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer):\n",
        "    # Initialize lists to track losses and tokens seen\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # Calculate loss gradients\n",
        "            optimizer.step() # Update model weights using loss gradients\n",
        "            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "        generate_and_print_sample(\n",
        "            model, tokenizer, device, start_context\n",
        "        )\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "331cb80c-2b99-482c-a5a3-c829965ebc18",
      "metadata": {
        "id": "331cb80c-2b99-482c-a5a3-c829965ebc18"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c0fb5cf-9b6b-42a5-8b42-396d6244e5b8",
      "metadata": {
        "id": "3c0fb5cf-9b6b-42a5-8b42-396d6244e5b8"
      },
      "outputs": [],
      "source": [
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=50, context_size=context_size\n",
        "        )\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a66acb76-a6da-4179-b468-df9bf0e4e6a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a66acb76-a6da-4179-b468-df9bf0e4e6a3",
        "outputId": "8089f1ed-6e24-43b1-805a-22cbe68bd1f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version: 2.5.1+cu121\n",
            "Ep 1 (Step 000000): Train loss 9.783, Val loss 9.927\n",
            "Ep 1 (Step 000005): Train loss 7.984, Val loss 8.334\n",
            "Every effort moves you,,,,,,,,,,,,.                                     \n",
            "Ep 2 (Step 000010): Train loss 6.753, Val loss 7.047\n",
            "Ep 2 (Step 000015): Train loss 6.113, Val loss 6.573\n",
            "Every effort moves you, and,, and, and,,,,, and, and,,,,,,,,,,,,,, and,,,, and,, and,,,,, and,,,,,,\n",
            "Ep 3 (Step 000020): Train loss 5.523, Val loss 6.491\n",
            "Ep 3 (Step 000025): Train loss 5.322, Val loss 6.386\n",
            "Every effort moves you, and to the picture.                      \"I, and the of the of the's the honour, and, and I had been, and I\n",
            "Ep 4 (Step 000030): Train loss 4.754, Val loss 6.361\n",
            "Ep 4 (Step 000035): Train loss 4.458, Val loss 6.262\n",
            "Every effort moves you of the to the picture to the of the picture--as I had been \" it was his \" I was the     \"I was his I had been the his pictures--and it the picture and I had been the picture of\n",
            "Ep 5 (Step 000040): Train loss 3.824, Val loss 6.198\n",
            "Every effort moves you know the \"Oh, and he was not the fact by his last word.         \"I was.      \"Oh, I felt a little a little the    \n",
            "Ep 6 (Step 000045): Train loss 3.339, Val loss 6.140\n",
            "Ep 6 (Step 000050): Train loss 2.847, Val loss 6.112\n",
            "Every effort moves you know; and my dear, and he was not the fact with a little of the house of the fact of the fact, and.                       \n",
            "Ep 7 (Step 000055): Train loss 2.329, Val loss 6.138\n",
            "Ep 7 (Step 000060): Train loss 2.064, Val loss 6.181\n",
            "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \"I looked--as of the fact, and I felt him--his back his head to the donkey. \"Oh, and_--because he had always _\n",
            "Ep 8 (Step 000065): Train loss 1.501, Val loss 6.180\n",
            "Ep 8 (Step 000070): Train loss 1.250, Val loss 6.182\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. \"Oh, and I had been me!\"  \"I looked up the moment--as Jack himself, and; and as I had been the man of the hour. \n",
            "Ep 9 (Step 000075): Train loss 0.978, Val loss 6.283\n",
            "Ep 9 (Step 000080): Train loss 0.700, Val loss 6.288\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 10 (Step 000085): Train loss 0.490, Val loss 6.336\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to the donkey again. I saw that, when Stroud laid in the first\n",
            "Training completed in 22.65 minutes.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# Add debugging prints\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0004)\n",
        "\n",
        "num_epochs = 10\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bc5c200-f1a4-4032-b1bf-1c270362dfdd",
      "metadata": {
        "id": "0bc5c200-f1a4-4032-b1bf-1c270362dfdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "outputId": "20539c77-594d-4e50-d2ed-c07007ff0764"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAEiCAYAAAA21pHjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABXJ0lEQVR4nO3dd3gU1RrA4d9uem+kkkKAQAoBQhViQyJFREEQ1KhgQyE0uSrYEGyIInJBLioqqFRRQVSKgPQaSqihhyRACi2d1D33j4UNCwgEEnYTvvd55mHnzJnZb4ck354zZ+ZolFIKIYQQQpglrakDEEIIIcS/k0QthBBCmDFJ1EIIIYQZk0QthBBCmDFJ1EIIIYQZk0QthBBCmDFJ1EIIIYQZk0QthBBCmDFJ1EIIIYQZk0QtRA1w7NgxNBoNCQkJpg5FCFHJJFELYSY0Gs01l1GjRpk6RCGECViaOgAhhF5aWprh9dy5cxk5ciQHDhwwlDk6OpoiLCGEiUmLWggz4ePjY1hcXFzQaDSGdS8vL8aPH4+/vz82NjY0bdqUJUuW/OuxysrKeP755wkNDSUlJQWA33//nWbNmmFra0vdunUZPXo0paWlhn00Gg3ffvst3bt3x97enpCQEBYuXGjYfu7cOWJjY/H09MTOzo6QkBCmTZv2rzH88ssvREZGYmdnh4eHBzExMeTn5xu2f/vtt4SFhWFra0toaCj/+9//jPZPTU2lV69euLq64u7uzqOPPsqxY8cM2/v27Uu3bt0YN24cvr6+eHh4EBcXR0lJyQ2fcyGqBSWEMDvTpk1TLi4uhvXx48crZ2dnNXv2bLV//371xhtvKCsrK3Xw4EGllFJJSUkKUDt27FCFhYWqe/fuKioqSmVmZiqllFqzZo1ydnZW06dPV0eOHFF///23qlOnjho1apThPQDl7++vZs2apQ4dOqQGDx6sHB0d1ZkzZ5RSSsXFxammTZuq+Ph4lZSUpJYtW6YWLlx41fhPnjypLC0t1fjx41VSUpLatWuXmjx5ssrNzVVKKTVjxgzl6+urfv31V3X06FH166+/Knd3dzV9+nSllFLFxcUqLCxMPf/882rXrl1q37596qmnnlINGzZURUVFSiml+vTpo5ydndUrr7yiEhMT1R9//KHs7e3VN998U7n/GUKYmCRqIczQ5Ynaz89PffTRR0Z1WrZsqQYMGKCUKk/Ua9euVe3bt1d33323ysrKMtRt3769+vjjj432/+mnn5Svr69hHVDvvPOOYT0vL08BavHixUoppbp27aqee+65G4p/27ZtClDHjh276vZ69eqpWbNmGZV98MEHqk2bNobYGjZsqHQ6nWF7UVGRsrOzU0uXLlVK6RN1UFCQKi0tNdR5/PHHVe/evW8oRiGqC7lGLYSZy8nJ4eTJk0RHRxuVR0dHs3PnTqOyJ598En9/f/755x/s7OwM5Tt37mT9+vV89NFHhrKysjIKCwspKCjA3t4egMaNGxu2Ozg44OzsTGZmJgD9+/enR48ebN++nQ4dOtCtWzfatm171ZibNGlC+/btiYyMpGPHjnTo0IGePXvi5uZGfn4+R44c4YUXXuCll14y7FNaWoqLi4sh3sOHD+Pk5GR03MLCQo4cOWJYj4iIwMLCwrDu6+vL7t27r3E2hah+JFELUYM89NBDzJgxg40bN/LAAw8YyvPy8hg9ejSPPfbYFfvY2toaXltZWRlt02g06HQ6ADp37kxycjKLFi1i2bJltG/fnri4OMaNG3fFMS0sLFi2bBkbNmzg77//ZtKkSbz99tts3rzZ8KVg6tSptG7d+or9LsbbvHlzZs6cecWxPT09byheIWoKSdRCmDlnZ2f8/PxYv3499913n6F8/fr1tGrVyqhu//79adSoEY888gh//fWXoX6zZs04cOAA9evXv6VYPD096dOnD3369OGee+7h9ddfv2qiBn3SjI6OJjo6mpEjRxIUFMT8+fMZNmwYfn5+HD16lNjY2Kvu26xZM+bOnYuXlxfOzs63FLMQ1Z0kaiGqgddff5333nuPevXq0bRpU6ZNm0ZCQsJVW5yDBg2irKyMhx9+mMWLF3P33XczcuRIHn74YQIDA+nZsydarZadO3eyZ88ePvzwwxuKYeTIkTRv3pyIiAiKior4888/CQsLu2rdzZs3s2LFCjp06ICXlxebN2/m1KlThvqjR49m8ODBuLi40KlTJ4qKiti6dSvnzp1j2LBhxMbG8tlnn/Hoo4/y/vvv4+/vT3JyMr/99htvvPEG/v7+N38yhahmJFELUQ0MHjyY7Oxs/vOf/5CZmUl4eDgLFy4kJCTkqvWHDh2KTqfjoYceYsmSJXTs2JE///yT999/n7Fjx2JlZUVoaCgvvvjiDcdgbW3Nm2++ybFjx7Czs+Oee+5hzpw5V63r7OzMmjVrmDBhAjk5OQQFBfH555/TuXNnAF588UXs7e357LPPeP3113FwcCAyMpKhQ4cCYG9vz5o1axg+fDiPPfYYubm51K5dm/bt20sLW9xxNEopZeoghBBCCHF18sATIYQQwoxJohZCCCHMmCRqIYQQwoxJohZCCCHMmCRqIYQQwoxJohZCCCHMmCTqfzF58mTq1KmDra0trVu3ZsuWLaYOySysWbOGrl274ufnh0ajYcGCBUbblVKMHDkSX19f7OzsiImJ4dChQ0Z1zp49S2xsLM7Ozri6uvLCCy+Ql5dnVGfXrl3cc8892NraEhAQwKeffnpFLPPmzSM0NBRbW1siIyNZtGhRpX/e22nMmDG0bNkSJycnvLy86Natm9F81KB/1nVcXBweHh44OjrSo0cPMjIyjOqkpKTQpUsX7O3t8fLy4vXXXzeazhJg1apVNGvWDBsbG+rXr8/06dOviKcm/g5MmTKFxo0b4+zsjLOzM23atGHx4sWG7XJ+K9cnn3yCRqMx3B8Pco5vioknBTFLc+bMUdbW1ur7779Xe/fuVS+99JJydXVVGRkZpg7N5BYtWqTefvtt9dtvvylAzZ8/32j7J598olxcXNSCBQvUzp071SOPPKKCg4PV+fPnDXU6deqkmjRpojZt2qTWrl2r6tevr5588knD9uzsbOXt7a1iY2PVnj171OzZs5WdnZ36+uuvDXXWr1+vLCws1Keffqr27dun3nnnHWVlZaV2795d5eegqnTs2FFNmzZN7dmzRyUkJKiHHnpIBQYGqry8PEOdV155RQUEBKgVK1aorVu3qrvuuku1bdvWsL20tFQ1atRIxcTEqB07dqhFixapWrVqqTfffNNQ5+jRo8re3l4NGzZM7du3T02aNElZWFioJUuWGOrU1N+BhQsXqr/++ksdPHhQHThwQL311lvKyspK7dmzRykl57cybdmyRdWpU0c1btxYDRkyxFAu57jiJFFfRatWrVRcXJxhvaysTPn5+akxY8aYMCrzc3mi1ul0ysfHR3322WeGsqysLGVjY6Nmz56tlFJq3759ClDx8fGGOosXL1YajUadOHFCKaXU//73P+Xm5maYd1gppYYPH64aNmxoWO/Vq5fq0qWLUTytW7dWL7/8cqV+RlPKzMxUgFq9erVSSn8urays1Lx58wx1EhMTFaA2btyolNJ/kdJqtSo9Pd1QZ8qUKcrZ2dlwPt944w0VERFh9F69e/dWHTt2NKzfSb8Dbm5u6ttvv5XzW4lyc3NVSEiIWrZsmbrvvvsMiVrO8c2Rru/LFBcXs23bNmJiYgxlWq2WmJgYNm7caMLIzF9SUhLp6elG587FxYXWrVsbzt3GjRtxdXWlRYsWhjoxMTFotVo2b95sqHPvvfdibW1tqNOxY0cOHDjAuXPnDHUufZ+LdWrS/1F2djYA7u7uAGzbto2SkhKjzx0aGkpgYKDR+Y2MjMTb29tQp2PHjuTk5LB3715DnWuduzvld6CsrIw5c+aQn59PmzZt5PxWori4OLp06XLFeZBzfHPkWd+XOX36NGVlZUY/JADe3t7s37/fRFFVD+np6QBXPXcXt6Wnp+Pl5WW03dLSEnd3d6M6wcHBVxzj4jY3NzfS09Ov+T7VnU6nY+jQoURHR9OoUSNA/9mtra1xdXU1qnv5+b3aebm47Vp1cnJyOH/+POfOnavRvwO7d++mTZs2FBYW4ujoyPz58wkPDychIUHObyWYM2cO27dvJz4+/opt8jN8cyRRC2GG4uLi2LNnD+vWrTN1KDVOw4YNSUhIIDs7m19++YU+ffqwevVqU4dVI6SmpjJkyBCWLVtmNM+5uDXS9X2ZWrVqYWFhccUoxIyMDHx8fEwUVfVw8fxc69z5+PiQmZlptL20tJSzZ88a1bnaMS59j3+rUxP+jwYOHMiff/7JypUrjaZz9PHxobi4mKysLKP6l5/fmz13zs7O2NnZ1fjfAWtra+rXr0/z5s0ZM2YMTZo04b///a+c30qwbds2MjMzadasGZaWllhaWrJ69WomTpyIpaUl3t7eco5vgiTqy1hbW9O8eXNWrFhhKNPpdKxYsYI2bdqYMDLzFxwcjI+Pj9G5y8nJYfPmzYZz16ZNG7Kysti2bZuhzj///INOp6N169aGOmvWrKGkpMRQZ9myZTRs2BA3NzdDnUvf52Kd6vx/pJRi4MCBzJ8/n3/++eeK7v/mzZtjZWVl9LkPHDhASkqK0fndvXu30ZehZcuW4ezsTHh4uKHOtc7dnfY7oNPpKCoqkvNbCdq3b8/u3btJSEgwLC1atCA2NtbwWs7xTTD1aDZzNGfOHGVjY6OmT5+u9u3bp/r166dcXV2NRiHeqXJzc9WOHTvUjh07FKDGjx+vduzYoZKTk5VS+tuzXF1d1e+//6527dqlHn300avenhUVFaU2b96s1q1bp0JCQoxuz8rKylLe3t7qmWeeUXv27FFz5sxR9vb2V9yeZWlpqcaNG6cSExPVe++9V+1vz+rfv79ycXFRq1atUmlpaYaloKDAUOeVV15RgYGB6p9//lFbt25Vbdq0UW3atDFsv3hrS4cOHVRCQoJasmSJ8vT0vOqtLa+//rpKTExUkydPvuqtLTXxd2DEiBFq9erVKikpSe3atUuNGDFCaTQa9ffffyul5PxWhUtHfSsl5/hmSKL+F5MmTVKBgYHK2tpatWrVSm3atMnUIZmFlStXKuCKpU+fPkop/S1a7777rvL29lY2Njaqffv26sCBA0bHOHPmjHryySeVo6OjcnZ2Vs8995zKzc01qrNz50519913KxsbG1W7dm31ySefXBHLzz//rBo0aKCsra1VRESE+uuvv6rsc98OVzuvgJo2bZqhzvnz59WAAQOUm5ubsre3V927d1dpaWlGxzl27Jjq3LmzsrOzU7Vq1VL/+c9/VElJiVGdlStXqqZNmypra2tVt25do/e4qCb+Djz//PMqKChIWVtbK09PT9W+fXtDklZKzm9VuDxRyzmuOI1SSpmmLS+EEEKI65Fr1EIIIYQZk0QthBBCmDFJ1EIIIYQZk0QthBBCmDFJ1EIIIYQZk0QthBBCmDFJ1NdQVFTEqFGjKCoqMnUoNZKc36ol57fqyTmuWnJ+9eQ+6mvIycnBxcWF7OxsnJ2dTR1OjSPnt2rJ+a16co6rlpxfPWlRCyGEEGZMErUQQghhxmr8fNSlpaXs2LEDb29vtNqKfS/Jzc0F4MSJE+Tk5FRFeHc0Ob9VS85v1ZNzXLVq8vnV6XRkZGQQFRWFpeW1U3GNv0YdHx9Pq1atTB2GEEIIcYUtW7bQsmXLa9ap8S1qb29vQH8yfH19TRyNEEIIAWlpabRq1cqQo66lxifqi93dvr6++Pv7mzgaIYQQotyNXJI16WCyNWvW0LVrV/z8/NBoNCxYsMBou1KKkSNH4uvri52dHTExMRw6dMg0wQohhBAmYNJEnZ+fT5MmTZg8efJVt3/66adMnDiRr776is2bN+Pg4EDHjh0pLCy8zZEKIYQQpmHSru/OnTvTuXPnq25TSjFhwgTeeecdHn30UQB+/PFHvL29WbBgAU888cTtDFUIIYQwCbO9Rp2UlER6ejoxMTGGMhcXF1q3bs3GjRv/NVEXFRUZPW7u4vB+IYS4EWVlZZSUlJg6DFHNWVlZYWFhUSnHMttEnZ6eDnDFiDhvb2/DtqsZM2YMo0ePrtLYhBA1j1KK9PR0srKyTB2KqCFcXV3x8fFBo9Hc0nHMNlHfrDfffJNhw4YZ1k+cOEF4eHjlHLysFFaMhrr3Qf2Y69cXQlQbF5O0l5cX9vb2t/zHVdy5lFIUFBSQmZkJcMu3Bpttovbx8QEgIyPD6ENmZGTQtGnTf93PxsYGGxsbw3qlPs1my9ewYSJs/xFeXg1udSrv2EIIkykrKzMkaQ8PD1OHI2oAOzs7ADIzM/Hy8rqlbnCzfdZ3cHAwPj4+rFixwlCWk5PD5s2badOmzW2Pp7RMx6Tcezlo2QAKs2Du01BccNvjEEJUvovXpO3t7U0ciahJLv483eqYB5Mm6ry8PBISEkhISAD0A8gSEhJISUlBo9EwdOhQPvzwQxYuXMju3bt59tln8fPzo1u3brc91rMFxXy3KY0+eYPIt3SD9N3w51Co2U9gFeKOIt3dojJV1s+TSRP11q1biYqKIioqCoBhw4YRFRXFyJEjAXjjjTcYNGgQ/fr1o2XLluTl5bFkyRJsbW1ve6xeTrZ83D2SNDx4sSAOpbGAXXNhyze3PRYhhBB3DpMm6vvvvx+l1BXL9OnTAf23kffff5/09HQKCwtZvnw5DRo0MFm8D0X60qOZPxt14UyyfFZfuPQtSN5gspiEEKKy1alThwkTJtxw/VWrVqHRaKp8xPz06dNxdXWt0vcwR2Z7jdpcjXokHH83O8bnxrDdJQZ0pfBzH8g5aerQhBB3GI1Gc81l1KhRN3Xc+Ph4+vXrd8P127ZtS1paGi4uLjf1fuLaJFFXkJOtFeN7NUWj0RCb8RQ5Lg0hPxN+fhZKi65/ACGEqCRpaWmGZcKECTg7OxuVvfbaa4a6SilKS0tv6Lienp4VGlhnbW1dKfcLi6uTRH0TWgW70/++epzHlqdyBqKzcYHj8bBkhKlDE0LcQXx8fAyLi4sLGo3GsL5//36cnJxYvHgxzZs3x8bGhnXr1nHkyBEeffRRvL29cXR0pGXLlixfvtzouJd3fWs0Gr799lu6d++Ovb09ISEhLFy40LD98q7vi13US5cuJSwsDEdHRzp16kRaWpphn9LSUgYPHoyrqyseHh4MHz6cPn36VHiw8JQpU6hXrx7W1tY0bNiQn376ybBNKcWoUaMIDAzExsYGPz8/Bg8ebNj+v//9j5CQEGxtbfH29qZnz54Veu/bRRL1TRoa04BGtZ3Zc96D8c5voNDA1u9h+0/X31kIYfaUUhQUl5pkUZV4N8mIESP45JNPSExMpHHjxuTl5fHQQw+xYsUKduzYQadOnejatSspKSnXPM7o0aPp1asXu3bt4qGHHiI2NpazZ8/+a/2CggLGjRvHTz/9xJo1a0hJSTFq4Y8dO5aZM2cybdo01q9fT05OzhUzKF7P/PnzGTJkCP/5z3/Ys2cPL7/8Ms899xwrV64E4Ndff+WLL77g66+/5tChQyxYsIDIyEhAP5h58ODBvP/++xw4cIAlS5Zw7733Vuj9bxezfeCJubO21DKhd1O6TFzHl6nBtG80gKjDk2HZuxDRDWycTB2iEOIWnC8pI3zkUpO89773O2JvXTl/nt9//30efPBBw7q7uztNmjQxrH/wwQfMnz+fhQsXMnDgwH89Tt++fXnyyScB+Pjjj5k4cSJbtmyhU6dOV61fUlLCV199Rb169QAYOHAg77//vmH7pEmTePPNN+nevTsAX375JYsWLarQZxs3bhx9+/ZlwIABgP7OoU2bNjFu3DjatWtHSkoKPj4+xMTEYGVlRWBgIK1atQIgJSUFBwcHHn74YZycnAgKCjLcgWRupEV9C+p7OfHWQ2EAPLk/mqzI56DPn5KkhRBmo0WLFkbreXl5vPbaa4SFheHq6oqjoyOJiYnXbVE3btzY8NrBwQFnZ2fDIzKvxt7e3pCkQf8YzYv1s7OzycjIMCRNAAsLC5o3b16hz5aYmEh0dLRRWXR0NImJiQA8/vjjnD9/nrp16/LSSy8xf/58w3X6Bx98kKCgIOrWrcszzzzDzJkzKSgwz4dYSYv6Fj3bJogV+zNZc/AUsSd6ML9WONamDkoIccvsrCzY935Hk713ZXFwcDBaf+2111i2bBnjxo2jfv362NnZ0bNnT4qLi695HCsrK6N1jUaDTqerUP3K7NK/EQEBARw4cIDly5ezbNkyBgwYwGeffcbq1atxcnJi+/btrFq1ir///puRI0cyatQo4uPjze4WMGlR3yKNRsNnPRvjam/F3pM5TFh+UL8hdQus/dy0wQkhbppGo8He2tIkS1WOnl6/fj19+/ale/fuREZG4uPjw7Fjx6rs/a7GxcUFb29v4uPjDWVlZWVs3769QscJCwtj/fr1RmXr1683mojJzs6Orl27MnHiRFatWsXGjRvZvXs3AJaWlsTExPDpp5+ya9cujh07xj///HMLn6xqSIu6Eng72zKmeyT9Z25nyuojdKhdTNMFXaCsGLzCoWFnU4cohBAAhISE8Ntvv9G1a1c0Gg3vvvvuNVvGVWXQoEGMGTOG+vXrExoayqRJkzh37lyFvqS8/vrr9OrVi6ioKGJiYvjjjz/47bffDKPYp0+fTllZGa1bt8be3p4ZM2ZgZ2dHUFAQf/75J0ePHuXee+/Fzc2NRYsWodPpaNiwYVV95JsmLepK0jnSl57N/VEK4v46TVGLlyH8Uahzj6lDE0IIg/Hjx+Pm5kbbtm3p2rUrHTt2pFmzZrc9juHDh/Pkk0/y7LPP0qZNGxwdHenYsWOFHhHdrVs3/vvf/zJu3DgiIiL4+uuvmTZtGvfffz+gnw966tSpREdH07hxY5YvX84ff/yBh4cHrq6u/PbbbzzwwAOEhYXx1VdfMXv2bCIiIqroE988jbrdFw1us+PHjxMQEEBqair+/v5V+l65hSV0/u9ajp87T88oX8b1igJ5AIAQZq+wsJCkpCSCg4NNMpeAAJ1OR1hYGL169eKDDz4wdTiV4lo/VxXJTdKirkROtlZ80bspWg38siONRXvS9RuUgr3zwQTdS0IIYY6Sk5OZOnUqBw8eZPfu3fTv35+kpCSeeuopU4dmdiRRV7KWddzpf7/+loS35u8mI6cQFvSHeX1hzaemDU4IIcyEVqtl+vTptGzZkujoaHbv3s3y5csJCwszdWhmRwaTVYEh7Ruw+uAp9pzI4bV5O/mh2T1od86GVWPAtyk0vPoDAoQQ4k4REBBwxYhtcXXSoq4CF59aZmOpZe2h0/xY0BZavqTf+Fs/OHPEtAEKIYSoNiRRV5H6Xk683UXfhTNm8X4ORb0JAXdBUTbMiYWiPBNHKIQQojqQRF2FnrkriPsaeFJUqmPIvH0UPzYNHL3hVCIsHKgfZCaEEEJcgyTqKnTxqWVu9lbsS8vhi8050OtH0FrqR4FvmGTqEIUQQpg5SdRVzMvZljGP6adV+2r1EbaUNYBOn+g3Ln8Pjq42YXRCCCHMnSTq26BTI18ev/DUslfnJpAT2QeaPAVKB788B1mppg5RCCGEmZJEfZu890gEAe52nMg6z6g/9sHD48GnMRScgblPQ0mhqUMUQtyh7r//foYOHWpYr1OnDhMmTLjmPhqNhgULFtzye1fWca5l1KhRNG3atErfoypJor5NHG0s+aKX/qllv20/wV+JWdB7Bti5Q1oCLH7d1CEKIaqZrl270qnT1Z/LsHbtWjQaDbt27arwcePj4+nXr9+thmfk35JlWloanTvLxEXXIon6NmpRx50B99cH9E8tS9d6Q8/vwMkXGj9h4uiEENXNCy+8wLJlyzh+/PgV26ZNm0aLFi1o3LhxhY/r6emJvb19ZYR4XT4+PtjY2NyW96quJFHfZkNiQois7UL2+RJe/2UnuuB2MHgH1Ik2dWhCiGrm4YcfxtPTk+nTpxuV5+XlMW/ePF544QXOnDnDk08+Se3atbG3tycyMpLZs2df87iXd30fOnSIe++9F1tbW8LDw1m2bNkV+wwfPpwGDRpgb29P3bp1effddykpKQH0002OHj2anTt3otFo0Gg0hpgv7/revXs3DzzwAHZ2dnh4eNCvXz/y8sqfO9G3b1+6devGuHHj8PX1xcPDg7i4OMN73QidTsf777+Pv78/NjY2NG3alCVLlhi2FxcXM3DgQHx9fbG1tSUoKIgxY8YAoJRi1KhRBAYGYmNjg5+fH4MHD77h974Z8gjR28zKQssXvZvy8KS1rD10mh82HuO56ODyCqnxkJ0CjXqYLkghRLni/IrvY2EDFhf+vJaVQlkRaLRgZXf941o73PDbWFpa8uyzzzJ9+nTefvttw1zO8+bNo6ysjCeffJK8vDyaN2/O8OHDcXZ25q+//uKZZ56hXr16tGrV6rrvodPpeOyxx/D29mbz5s1kZ2cbXc++yMnJienTp+Pn58fu3bt56aWXcHJy4o033qB3797s2bOHJUuWGOaKdnFxueIY+fn5dOzYkTZt2hAfH09mZiYvvvgiAwcONPoysnLlSnx9fVm5ciWHDx+md+/eNG3alJdeeumGztt///tfPv/8c77++muioqL4/vvveeSRR9i7dy8hISFMnDiRhQsX8vPPPxMYGEhqaiqpqfpBv7/++itffPEFc+bMISIigvT0dHbu3HlD73uzzDpRl5WVMWrUKGbMmEF6ejp+fn707duXd955p0KTi5ub+l6OvP1QGO/+vpcxi/cTXb8WDbyd4PQh+Kk7lOSDnRvUe8DUoQohPvar+D6PT4eI7vrX+//QT8oTdDc891d5nQmR+sGklxuVXaG3ev755/nss89YvXq1YR7madOm0aNHD1xcXHBxceG1114z1B80aBBLly7l559/vqFEvXz5cvbv38/SpUvx89Ofi48//viK68rvvPOO4XWdOnV47bXXmDNnDm+88QZ2dnY4OjpiaWmJj4/Pv77XrFmzKCws5Mcff8TBQf+F5csvv6Rr166MHTsWb29vANzc3Pjyyy+xsLAgNDSULl26sGLFihtO1OPGjWP48OE88YT+kuPYsWNZuXIlEyZMYPLkyaSkpBASEsLdd9+NRqMhKCjIsG9KSgo+Pj7ExMRgZWVFYGDgDZ3HW2HWXd9jx45lypQpfPnllyQmJjJ27Fg+/fRTJk2q/g8KefquIO5v6ElxqY6hcxIoLtWBez0IfwSCoiGgtalDFEJUA6GhobRt25bvv/8egMOHD7N27VpeeOEFQN/g+eCDD4iMjMTd3R1HR0eWLl1KSkrKDR0/MTGRgIAAQ5IGaNOmzRX15s6dS3R0ND4+Pjg6OvLOO+/c8Htc+l5NmjQxJGmA6OhodDodBw4cMJRFRERgYWFhWPf19SUzM/OG3iMnJ4eTJ08SHW18uTE6OprExERA372ekJBAw4YNGTx4MH///beh3uOPP8758+epW7cuL730EvPnz6e0tLRCn7OizLpFvWHDBh599FG6dOkC6L+lzZ49my1btpg4slun0Wj4tEdjOk5Yw760HMYvO8iIzqHwyCQoKzbuIhNCmM5bJyu+j8Ulg6NCu+qPobmsXTR0963FdYkXXniBQYMGMXnyZKZNm0a9evW47777APjss8/473//y4QJE4iMjMTBwYGhQ4dSXFxcae+/ceNGYmNjGT16NB07dsTFxYU5c+bw+eefV9p7XMrKyspoXaPRoNPpKu34zZo1IykpicWLF7N8+XJ69epFTEwMv/zyCwEBARw4cIDly5ezbNkyBgwYYOjRuDyuymLWLeq2bduyYsUKDh48CMDOnTtZt27dNYfyFxUVkZOTY1hyc3NvV7gVpn9qmX5E5tdrjrDp6BnQWpQnaaVg9WdwaLkJoxTiDmftUPHF4pI2kIWlvuzyL9//tu9N6NWrF1qtllmzZvHjjz/y/PPPGy4Prl+/nkcffZSnn36aJk2aULduXcPf1BsRFhZGamoqaWlphrJNmzYZ1dmwYQNBQUG8/fbbtGjRgpCQEJKTk40/rrU1ZWVl132vnTt3kp9ffv1+/fr1aLVaGjZseMMxX4uzszN+fn5XTLG5fv16wsPDjer17t2bqVOnMnfuXH799VfOnj0LgJ2dHV27dmXixImsWrWKjRs3snt35X3xupxZJ+oRI0bwxBNPEBoaipWVFVFRUQwdOpTY2Nh/3WfMmDGG6zIuLi5GJ94cdWrkQ68W+qeW9Z+xjb0nL7k+tedXWPkhzHkKjqw0XZBCCLPm6OhI7969efPNN0lLS6Nv376GbSEhISxbtowNGzaQmJjIyy+/TEZGxg0fOyYmhgYNGtCnTx927tzJ2rVrefvtt43qhISEkJKSwpw5czhy5AgTJ05k/vz5RnXq1KlDUlISCQkJnD59mqKioiveKzY2FltbW/r06cOePXtYuXIlgwYN4plnnjFcn64Mr7/+OmPHjmXu3LkcOHCAESNGkJCQwJAhQwAYP348s2fPZv/+/Rw8eJB58+bh4+ODq6sr06dP57vvvmPPnj0cPXqUGTNmYGdnZ3Qdu7KZdaL++eefmTlzJrNmzWL79u388MMPjBs3jh9++OFf93nzzTfJzs42LPv27buNEd+ckV0jaOLvwrmCEp6aupndxy8k67BHoEFn/YjR2U9C0lrTBiqEMFsvvPAC586do2PHjkbXk9955x2aNWtGx44duf/++/Hx8aFbt243fFytVsv8+fM5f/48rVq14sUXX+Sjjz4yqvPII4/w6quvMnDgQJo2bcqGDRt49913jer06NGDTp060a5dOzw9Pa96i5i9vT1Lly7l7NmztGzZkp49e9K+fXu+/PLLip2M6xg8eDDDhg3jP//5D5GRkSxZsoSFCxcSEhIC6Eewf/rpp7Ro0YKWLVty7NgxFi1ahFarxdXVlalTpxIdHU3jxo1Zvnw5f/zxBx4eHpUa46U0SpnvXIsBAQGMGDGCuLg4Q9mHH37IjBkz2L9//w0d4/jx4wQEBJCamoq/v39VhXrLcgpL6Pv9FranZOFka8mPz7ciKtANSov081cfXgZWDvD0rxB05UAOIcTNKywsJCkpieDgYGxtbU0djqghrvVzVZHcZNYt6oKCArRa4xAtLCwqddCAuXC2teLHF1rTso4buYWlPPPdFrYeOwuWNvpHjdZtp79ta+bj+nuthRBC3BHMOlF37dqVjz76iL/++otjx44xf/58xo8fT/fu3U0dWpVwtLHkh+db0aauB3lFpTz7/Rb9ADMrW3hiFtS5B4pzYcZjcGK7qcMVQghxG5h1op40aRI9e/ZkwIABhIWF8dprr/Hyyy/zwQcfmDq0KmNvbcn3fVtyT0gtCorL6DttC+sPnwZre3hqLgS2haIc+KkbpFXt03CEEEKYnlknaicnJyZMmEBycjLnz5/nyJEjfPjhh1hbW5s6tCplZ23B1GdbcH9DTwpLdDw/PZ5VBzL1t27E/qx/GEphNvzYDTL2mjpcIYQQVcisE/WdzNbKgq+faU5MmBdFpTr6/biNFYkZYOMEsfOgdnM4fxZ+eAQyb2xgnRBCiOpHErUZs7G04H+xzekU4UNxmY5XZmxj6d50sHWBp38D3yZQcFrfDV6Ud93jCSGurSYOVBWmU1k/T2b9CFEB1pZaJj0VxatzE/hzVxpxM7fz3yei6NLYF55ZoE/Sdw0AG0dThypEtWVtbY1Wq+XkyZN4enpibW1drSf+EaallKK4uJhTp06h1Wpv+XKtJOpqwMpCy4TeTbGy0DJ/xwkGzd5Oqa4pjzatDS/+Y/y4QiFEhWm1WoKDg0lLS+PkyZt4trcQV2Fvb09gYOAVtxlXlPyFryYsLbSMe7wJFloNv2w7zqtzEygpU/RsfsmN8rnp8PtAeHg8uAaaLlghqiFra2sCAwMpLS297jOphbgeCwsLLC0tK6VnRhJ1NWKh1c+4ZWWhYfaWVF7/ZSelZTqeaHUhKf8xRP8Es/n9jee9FULcEI1Gg5WVVZXNgiTEzZBEXc1otRo+6haJlYWWHzcmM+K33ZToFM/cFQRdxkNJAXSdaOowhRBCVBJJ1NWQVqth9CMRWGq1fL8+iXcX7KG0TMdz0cHQ5w/jyjod3OL1ESGEEKYjf8GrKY1Gw7sPh/HyfXUBGP3HPqauOWpc6cBimHo/5J++/QEKIYSoFJKoqzGNRsOITqEMeqA+AB8tSmTyysP6jaVFsPgN/WNGf3wUjm8F850oTQghxL+QRF3NaTQa/tOhIa/GNADgs6UHmLD8IMrCWv9QFEdvyNgD37aHya1g3ReQk2biqIUQQtwoSdQ1xJCYEF7v2BCACcsP8fnfB1Ee9eG5xdC4N1jawemDsHwUfBEOM3rAnt+gpNC0gQshhLgmGUxWg8S1q4+1hZaPFiXy5crDlJTpGNE5FM1j38BD42DfAkiYBSkb4fBy/WLrAo16QtNYqN0M5GlMQghhVqRFXcO8dG9dRnUNB+DrNUf54M9ElFJg6wzNnoXnl8Cg7XDPa+BcWz8L19bv4NsH9A9LEUIIYVYkUddAfaOD+bBbIwC+X5/EyN/3Uqa7ZCCZRz1o/y4M3a1/XnhkL7C0hTrR5XXyTsHeBfpBaUIIIUxGur5rqKfvCsLaQsvw33bx06ZkNh09w9CYBnRu5INWe6F7W2sB9drpl8JxYHHJg+N3zYG/34F6D8Az803zIYQQQkiLuibr1TKACb2b4mxryaHMPOJmbeehiWtZsicNne6yW7VsXcDKrnzd0hac/CD04fKy8+dgwyTIzbg9H0AIIQQapWr2zbXHjx8nICCA1NRU/P39r79DDZRTWML365L4bm0SuUWlAIT7OvPqgw2ICfP694fG68r0i+WFlvaWqbDoNdBYQEgHaPoUhDxonOCFEEJcV0VykyTqO0h2QQnfrjvK9+uSyC/Wzw4UWduFYQ824P6Gntef5WX/X/r7sI/HG5db2oKtq75Vbudq/Nq3KUTFltdN3qifO7tWw/IvAEIIcYeRRH0JSdRXOpdfzNS1R5m+4RgFFxJ20wBXXn2wAfeG1Lp+wj51EBJmwq65kHudh6eEPwq9ftS/1ungAw9QOvjPAXDy0Zev+AD2/Hplkrf3AAdPsK8FDpe+rgUWMruREKL6qkhuksFkdyA3B2ve6BTKC3cH882ao/yw8RgJqVn0+X4LzYPcGPZgA9rW8/j3hO3ZAB4cDTGjoCgHzmdBYZb+Vq+Lr89fWPcMLd+v9Dy419OX27qWl+ecgHNJcK4CH6JBZ3hqTvn60rf1XfB3DQB7d31Z/ml91729B1jIj7oQonqSFrUgM7eQr1cfZcamZIpKdQC0CnZn2IMNuKuuR9UHkJUCOScvS/jnoOCMPtnmnyp/XXBa3yKP6A6PT9fvryuDD2pd2VJf8iZs+p/+ta2rviXu4Kl/bWGlX7RW+iRuYa1/7dkQWr5QHtvG/4Eq0z8Q5uIXgLRdcOqAfj+tlX7fi69tnPRfDOw9wNq+6s+dEKJakha1qBAvJ1vefTicfvfWZcqqI8zanMKWpLM88c0m2tbz4NUHG9CyjnvVBeAaqF9uhE6nT+a60kvKSuG+4fpEbn/JF4uS86DR6hN4YZZ+OXP42sev94Bxol75MRTnQsOHyhP13t/01+qvx9L2QtJ2B58m0G1y+bY9v+r/rduu/LhKyZPhhKgKJech+4T+98ujXnn5gSX6RkBJAZQW6uuVFOgfrVxSoF8vPX+h/Ly+h/Dh8bc9fEnUwsDb2ZZRj0Tw8n11mbzyMHPjU9lw5AwbjmzknpBavPpgA5oFupk2SK22PLFdZGkD94+4sm7XCdDlc31LPf+UvjWef0q/riuFshLQlVz498K6e7DxMSJ76n9hbV3Ky9zqQPB9lx2jFMqKoShX/z5lxfpf/JwT+sXKwfi4S9+B3JPw0sryz7NhIqz+TL9+sVVuWNzAxhmsHfWD8awd9a13O3eoVf/WzqkQlUUpKM7XJ7+C01Bw9pLesDPGy/lz+t6wTmP0d48AHFwKi14H/xbQ8/vy4/6vrX4f1IVZAC/9V3fJa/T/6kqh0yfQvI9+/5SN8FN38G4E/deXH3fpW3D2yI1/vrLiWzk7N00StbiCr4sdH3aL5JX76jF55RHmbU1l7aHTrD10mvsbevJqTAOaBLiaOswbo7W4MBDtJrvwu064sqx5X/3yb4z+WJ3R/7GytDGuE9RWPxDP0bu8rOCMvvVenAtZyTcWn2coxG0uX//6Xn3L4cnZENBKX3bwb9g5+0KCd7ok0V+67qD/MmF9YbFxBDsTfymrSdSF5HHplUYLq/IeFF1ZeY+K1qJ8n8uvTF7a43J578uldQ3H1aHPXhr9l9yL9cpKLu502b5XWS/K0yddlwD9o4gBUuNh9zyoFQKtXtKXlZXCGH99C7QiinLLXxfn63/2XS7rCs5Lv5CoK6CkoPy1tSPYuIDVZZejgtqCe139+BbDYq/vDbOyv7B+yWsHr4rFUEnM/hr1iRMnGD58OIsXL6agoID69eszbdo0WrRocUP7yzXqW5d6toBJ/xzi1+0nDI8ijQnzYmhMAxrVdrnO3uKGFeVCXuaVLY+Lyb4oF4rz9H84i/P067VC4Olfy48xPlzfgu+3Cvyi9GXrvtDPmlYRzrVh2L7y9dlPwZlD0GU8BN+jL0veCAkz9H8ELyZ4a0f9H7WLry2tL1zDtwatpf4PoNclAwwLcwCl36eqR/KXleoHP+ou9H6UlRj3qlx8XVqk7w0pLdR3gTbqUX4rYeIfkLpZf4mk3gP6srNHYdEb5V2nhn+LLnSbFur/VTrjeOLi9QMzAf75ENZ8Bq36wUOf6ctyM+DzBhX/nM8t1icggE1fwZLhEPEYPD5NX6Yrg/dv4lLWU/OgQQf964TZsOCVK59c+EmgfoyJhY1+TMilvUKGdXf93Rt2bvqfCc+G+m2g/zk/c0TfW3Tpz0n6Hv1YETT6LyEabflrLqwbXl/4smNfS/+F00zVmGvU586dIzo6mnbt2rF48WI8PT05dOgQbm7yTf92CnC359OeTRhwf30m/nOIBTtOsDwxk+WJmTwY7s2A++sRZeou8ZrAxkm/XHoNraKeX6pPRu51y8vqtoNOdvqWelHeZQn/QllJgb41U5wHxQX6RHupc0n6aVJVWXnZqUTYMaOCn9EF3kwpX//5GTi6Ch6bCo176cv2L4LfXrpksJ/1hcF/1pcM3LO85JJFsT75DN5eftxfX4LEhdDx4/IxB8nr4cdHKhYv6B/uY3mhR+bIP7D1e33vw8VEXVIIh5dV/LjVhkafXMsuee6/b2O45z/gGWZctf9G/WUia4ebG29h737lpS0An0YVP1YNYtaJeuzYsQQEBDBt2jRDWXBw8DX2EFWpTi0HxvdqSly7+kxccYiFO0+ybF8Gy/ZlcFddd/rfX//G7sMWVcc14Moyv6b6pSJ0ZcbrPb7VX1P0jrjkuM3ggXcvS/L5+kR/cb2s+JKl9MoWzsUu2Etb06WF+n0rSld2Sbdxmf44l15TvPQ9Lo7yNxr9f3Gx0Xd3Wl5YLlW3nb71f/GyAoBLbXj0f/rLG1Z2F7pN7cr3t7LVzwdvaXOhJXiBjVP567uH6W8tvPQSiYMnvJFUvm7U+XlZR+ilAxFtnMvLm/fVfwG69Dn+Gi2MuOTLEhf2M/zeXrZuaVt+Xi/yjjD+Wbj0XIhKZ9Zd3+Hh4XTs2JHjx4+zevVqateuzYABA3jppZf+dZ+ioiKKisq/+Z04cYLw8HDp+q4ChzNz+Wr1URbsOEHphS7xcF9n+t9fj4cifbHQSsIW16HT6bubNRbl97oX50NexoXu6EuSvOH1hZb05UnWv1X5ddi8TH2itnMv/3Kg0+m7n7UWMrpemFyNeTKZra3+2+ywYcN4/PHHiY+PZ8iQIXz11Vf06dPnqvuMGjWK0aNHX1EuibrqnMw6z7drk5i9JYXzJfqWWJCHPf3urUuPZv7YWllc5whCCHFnqTGJ2tramhYtWrBhwwZD2eDBg4mPj2fjxo1X3Uda1KZzLr+YHzYeY/qGY2QV6Ls0azna8MLdwcTeFYizrTz2UwghoGKJ2qynufT19SU8PNyoLCwsjJSUlH/ZA2xsbHB2djYsTk5O/1pXVC43B2uGxjRgw4gHGPlwOH4utpzOK2Lskv1Ej/mHsUv2k5lbaOowhRCiWrmpRJ2amsrx48cN61u2bGHo0KF88803lRYYQHR0NAcOHDAqO3jwIEFBQZX6PqJy2Vtb8vzdwax+ox2fP96EEC9HcotKmbLqCHePXcnb83eTfCbf1GEKIUS1cFOJ+qmnnmLlypUApKen8+CDD7Jlyxbefvtt3n///UoL7tVXX2XTpk18/PHHHD58mFmzZvHNN98QFxdXae8hqo6VhZYezf1ZOvRepj7bgmaBrhSX6pi5OYV241YxaPYO9p7MNnWYQghh1m4qUe/Zs4dWrfS3J/z88880atSIDRs2MHPmTKZPn15pwbVs2ZL58+cze/ZsGjVqxAcffMCECROIjY29/s7CbGi1Gh4M9+bX/m35+eU2tGvoiU7BHztP0mXiOvp8v4VNR89gxsMlhBDCZG7qPuqSkhJsbPT3+y1fvpxHHtE/RCA0NJS0tOvMT1xBDz/8MA8//HClHlOYhkajoVWwO62CW7HvZA5frznCHztPsvrgKVYfPEVUoCuv3FePB8O80cqtXUIIAdxkizoiIoKvvvqKtWvXsmzZMjp16gTAyZMn8fC4DdMiimov3M+Z/z4RxarX2vHMXUHYWGrZkZLFyz9t48EvVjNvaypn803zAHwhhDAnN3V71qpVq+jevTs5OTn06dOH77/Xz3Ly1ltvsX//fn777bdKD/RmybO+q4dTuUVM35DEjxuTyS0sn8LS3cGa+p6O1PNypL6XI/U8Hajv5Yifi520uoUQ1dZtuY+6rKyMnJwco+duHzt2DHt7e7y8TDPDyNVIoq5ecgtLmLU5hTnxqSSd/veR4XZWFtTzcqC+pz6BX1yCPBywsjDruw6FEKLqJ+U4f/48SilDkk5OTmb+/PmEhYXRsWPHmzmkEAA42Vrx8n31ePm+epwvLuPIqTyOnMrjcGb5cuxMPudLythzIoc9J3KM9rfUagjysDdK3vU9najr6YCDjVk/2l4IIa7qpv5yPfroozz22GO88sorZGVl0bp1a6ysrDh9+jTjx4+nf//+lR2nuAPZWVvQqLbLFVNplpTpSDlbYEjcRzLzOHxK/29+cRlHTuVz5FQ+S/dmGO3n52JLPS9HIvxceKJlAHVqXTZDlBBCmKGbStTbt2/niy++AOCXX37B29ubHTt28OuvvzJy5EhJ1KJKWVloqefpSD1PRzpeMoGPUoq07EJ98r6kFX7kVB6n84o5mV3IyexC1h46zddrjtAh3Jt+99aledBNzM0rhBC3yU0l6oKCAsOjOf/++28ee+wxtFotd911F8nJyZUaoBA3SqPR4Odqh5+rHfc28DTallVQbEjcS/ems/LAKZbuzWDp3gyiAl3pd09dOkT4yIxfQgizc1OjburXr8+CBQtITU1l6dKldOjQAYDMzEycnZ2vs7cQt5+rvTUt6rjzRKtApj3XimWv3kvvFgFYW+hvC+s/czvtxq1i+vok8otKr39AIYS4TW4qUY8cOZLXXnuNOnXq0KpVK9q0aQPoW9dRUVGVGqAQVSHE24mxPRuzbkQ7Bj1QH1d7K1LOFjDqj320/eQfPl2yn8wcmUBECGF6N317Vnp6OmlpaTRp0gTthcnat2zZgrOzM6GhoZUa5K2Q27PEjSgoLuXXbcf5bl0Sx84UAGBloeHRprV56Z66NPSRWdiEEJXnts5HfXEWLXNNgpKoRUWU6RTLEzOYuuYoW5PPGcrvbeBJv3vqEl3fA41GrmMLIW5Nlc9HrdPpeP/993FxcSEoKIigoCBcXV354IMP0Ol0NxW0EObAQquhY4QPv/Rvy28D2vJQpA9aDaw5eIqnv9tM5/+u5ddtxykulZ9zIcTtcVOjvt9++22+++47PvnkE6KjowFYt24do0aNorCwkI8++qhSgxTCFJoFuvG/2OaknCng+/VJ/Lw1lf3pufxn3k4+Xbqfvm2Deap1IC52VqYOVQhRg91U17efnx9fffWVYdasi37//XcGDBjAiRMnKi3AWyVd36KyZBUUM3NzCj9sOEZmbhEADtYW9GoZwPPRwQS425s4QiFEdVHlXd9nz5696oCx0NBQzp49ezOHFMLsudpbE9euPmuHt2Pc401o6O1EfnEZ09Yf477PVhI3azurD54iT27vEkJUopvq+m7SpAlffvklEydONCr/8ssvady4caUEJoS5srG0oGdzf3o0q83aQ6eZuvYoaw+d5q9dafy1Kw0LrYZwX2da1HGjZR13WgS54eVsa+qwhRDV1E11fa9evZouXboQGBhouId648aNpKamsmjRIu65555KD/RmSde3uB0S03L4YcMx1h46zYms81dsD/Kwp0WQOy3ruNEy2J26tRxk9LgQd7DbcnvWyZMnmTx5Mvv37wcgLCyMfv368eGHH/LNN9/czCGrhCRqcbudzDrP1uRzbD12lvhj59ifnsPlv2XuDta0CLrQ4q7jRoSfC9aWMj2nEHeK23of9aV27txJs2bNKCsrq6xD3jJJ1MLUss+XsD2lPHEnpGZdcXuXrZWWpgGutKrjTos67kQFuuJkK6PJhaipqnw+aiHEjXOxs6JdQy/aNfQCoKhUP5f2xcS9NfksWQUlbDp6lk1H9YMxtRoI83U2tLhb1nHHW65zC3FHkkQtxG1mY2lB8yA3mge58fJ9oNMpjp7OI/7YOeKTzhKffJbUs+fZezKHvSdzmL7hGAAxYV4MjWlwxfzcQoiaTRK1ECam1Wqo7+VEfS8nnmwVCEB6diFbk8+y9dg54o+dZV9aDssTM1memMmD4d4MjQkhwk8SthB3ggol6scee+ya27Oysm4lFiHEBT4utjzc2I+HG/sBcORUHl/+c5jfE06wbF8Gy/Zl0DHCmyHtGxDuJ1PLClGTVShRu7hc+xu8i4sLzz777C0FJIS4Uj1PR77o3ZS4dvWZ9M8hFu48ydK9GSzdm0GnCB+GxIQQ5isJW4iaqFJHfZsjGfUtaqLDmblMXHGYP3adNNz69VCkD4PbhxDqIwlbCHNX5Y8QNZVPPvkEjUbD0KFDTR2KECZV38uJiU9G8ffQe3m4sS8aDSzanU6nCWuJm7mdgxm5pg5RCFFJqk2ijo+P5+uvv5ZHlApxiRBvJ758qhlLh95Ll0hfAP7anUbHCWsYOGs7hyRhC1HtVYtEnZeXR2xsLFOnTsXNzc3U4Qhhdhp4OzE5thlLht7DQ5E+KAV/7kqjw4Q1DJq9g8OZkrCFqK6qRaKOi4ujS5cuxMTEXLduUVEROTk5hiU3V/5AiTtHqI8z/4ttzuIh99ApQp+w/9h5kge/WMOQOTs4nJln6hCFEBVk9vdRz5kzh+3btxMfH39D9ceMGcPo0aOrOCohzFuYrzNfPdOcvSezmbjiEEv3ZvB7wkn+2HmSR5r4Mbh9CHU9HU0dphDiBph1izo1NZUhQ4Ywc+ZMbG1v7PGJb775JtnZ2YZl3759VRylEOYrws+Fr59pwZ+D7ubBcG90ChYknCRm/GqGzU0g6XS+qUMUQlyHWd+etWDBArp3746FhYWhrKysDI1Gg1arpaioyGjb1cjtWUKU23MimwnLD7E8MQPQP1P8kSZ+xN4VRIsgN5l6U4jbxGSzZ1W23NxckpOTjcqee+45QkNDGT58OI0aNbruMSRRC3Gl3cezmbD8ICv2ZxrK6tZy4PEWAfRoVhsvmQBEiCpVY2bPcnJyuiIZOzg44OHhcUNJWghxdZH+LnzXtyW7jmcxY1Myf+5K4+jpfMYu2c+4vw/QrqEnj7cI4IFQL6wszPoKmRA1nlknaiFE1Wrs78qnPV0Z2TWCRbvSmLs1lW3J5wwTgNRytOaxZv70auFPfS8nU4crxB3JrLu+K4N0fQtRMYcz85i3NZVft5/gdF6RobxZoCu9WgTwcBM/HG3kO74Qt6LGXKOuDJKohbg5JWU6Vu7P5Oetx1l5IJMynf5PhZ2VBV0a+9K7ZYAMQBPiJtWYa9RCCNOxstDSIcKHDhE+ZOYU8tuOE/wcn8rR0/n8su04v2w7LgPQhLgNpEUthLhhSim2JZ9jbnwqf+1Oo6C4DAALrUYGoAlRAdL1fQlJ1EJUjbyiUv7adZKftx5nW/I5Q7kMQBPi+iRRX0IStRBV73BmLvO2HufX7cc5nVdsKG8W6MoTrQLpEumLgwxAE8JAEvUlJFELcfuUD0BLZeWBU4YBaI42lnRt4scTLQNo7O8iA9DEHU8GkwkhTOLyAWi/bD/O3PhUks8UMHtLCrO3pBDq48QTLQPoFlUbV3trU4cshNmTFrUQokrpdIrNSWeZG5/Coj3pFJfqALC21NK5kQ+9WwZwV7AHWq20ssWdQ1rUQgizodVqaFPPgzb1PBhdUMKChBPM3pLC/vRcfk84ye8JJwnysKdXiwAeb+4vt3kJcRlpUQshbjulFLtPZDMnPpWFCSfJKyoFLt7m5cUTLQO4v6EnlnKbl6ihpEUthDBrGo2Gxv6uNPZ35Z0uYfy1K4258alsTT7H8sQMlidm4OVkw+Mt/OndIpBAD3tThyyEyUiLWghhNg5n5jI3Xv+c8bP55bd5Rdf3oHfLQDqEe2Nrde056IWoDuT2rEtIohai+iku1bE8MYM58amsPXSKi3+lXO2t6B5VmydaBtLQRx6mIqov6foWQlRr1pZaHor05aFIX46fK2De1uPM25rKyexCpq0/xrT1x2gR5MbTdwXROdIHG0tpZYuaS1rUQohqoUynWHvoFHPjU1m2L4PSCw9TcXew5vEW/sS2CpJr2aLakBa1EKLGsdBquL+hF/c39CIzp5A58anM3pJCWnYhX68+yjdrjnJfA0+ebh1Eu1AvLOS+bFFDSItaCFFtlZbp+Gd/JjM2p7Dm4ClDeW1XO55sFUDvloF4OtmYMEIhrk4Gk11CErUQd4Zjp/OZtSWFn7emklVQAoCVhYaOET48fVcQrYPd5RnjwmxIor6EJGoh7iyFJWX8tSuNGZuT2ZGSZSgP8XIktnUgjzX3x9nWynQBCoEkaiOSqIW4c+05kc3Mzcks2HGS8yVlANhZWdAtyo/Y1kE0qu1i4gjFnUoS9SUkUQshcgpLmL/9BDM2JXMoM89QHhXoytOtg+jS2FcepCJuK0nUl5BELYS4SCn9TF4zNiWzdG86JWX6P3+u9lY83tyf2NZB1KnlYOIoxZ1Abs8SQoir0Gg03FXXg7vqenAqt4ift6Yya3MKJ7LOM3VtElPXJtE8yI1mga40CXClib8r/m52MghNmJQkaiHEHcnTyYa4dvV55b56rNyfyYzNyaw+eIptyefYlnzOUM/DwdqQtJsEuNDE3xU3B2sTRi7uNJKohRB3NAuthphwb2LCvUk9W8DmpLPsTM1i5/EsEtNyOJNfzD/7M/lnf6ZhnyAP+wuJ25WmAS5E+LnINW5RZcw6UY8ZM4bffvuN/fv3Y2dnR9u2bRk7diwNGzY0dWhCiBoowN2eAHd7ejbXXzMsLCkjMS3nQuLOZmdqFkdP55N8poDkMwUs3HkS0Cf7UB8nfeK+kMDreznK09FEpTDrRL169Wri4uJo2bIlpaWlvPXWW3To0IF9+/bh4CADPoQQVcvWyoKoQDeiAt0MZdkFJew6kcXO1CwSUrNJSM3idF4Re0/msPdkDrM2pwBgb21BZG0Xo25zfzd5FrmouGo16vvUqVN4eXmxevVq7r333hvaR0Z9CyGqklKKtOxCfeI+rk/gu49nk19cdkXdpgGuvHhPMJ0ifLC00JogWmEuauyo7+zsbADc3d3/tU5RURFFRUWG9dzc3CqPSwhx59JoNPi52uHnakfnSF9AP9PXkVN5JKRmGa5370/LJSE1i4GzdlDb1Y7nouvQu2UATvKUNHEd1aZFrdPpeOSRR8jKymLdunX/Wm/UqFGMHj36inJpUQshTOlUbhEzNiXz06ZkzuYXA+BkY8kTrQLoGx1MbVc7E0cobqca+cCT/v37s3jxYtatW3fND3V5i/rEiROEh4dLohZCmIXCkjLm7zjBt2uPcuRUPqAfjPZQpC8v3h1MkwBX0wYobosal6gHDhzI77//zpo1awgODq7QvnKNWghhjnQ6xeqDp/h23VHWHz5jKG9Zx40X76lLTJi3jBqvwWrMNWqlFIMGDWL+/PmsWrWqwklaCCHMlVaroV2oF+1Cvdh7Mpvv1iXxx86TxB87R/yxbQR52PN8dDA9m/vjYGPWf6pFFTPrFvWAAQOYNWsWv//+u9G90y4uLtjZ3dj1HGlRCyGqi4ycQn7YcIyZm1PIPq+fU9vFzoqnWgfSp00dfFxsTRyhqCw1puv7356vO23aNPr27XtDx5BELYSobgqKS/l123G+W5fEsTMFAFhqNTzSxI8X7gkmwk+m56zuakyirgySqIUQ1VWZTrEiMYNv1yax5dhZQ3mbuh68dG8w9zfwQivXsaulGnONWggh7mQWWg0dInzoEOHDztQsvluXxF+709h49Awbj56hrqcDL9wdTI9m/vKs8RpMWtRCCFGNnMg6zw8bjjF7cwq5RaWA/n7sxgEuNKrtQuSFJdDdXqbnNGPS9X0JSdRCiJoor6iUufGpfL8uiRNZ56/Y7mRrSSM/FyL9yxN4kLu9dJWbCen6FkKIGs7RxpIX7g6mb9s6JKblsOdENrtPZLPnRDaJ6bnkFpYausgvcrKxJKK2M5G1y5N3HQ8HSd5mThK1EEJUYxZaDY0uJN4nLpSVlOk4mJFrSN67T+SQmJZDblEpm46eZdPR8oFpTjaWhPvpk/fF1newJG+zIolaCCFqGCsLLRF+LkT4udC7pb6spEzHoYy88pb3yWz2ndQn781JZ9mcVJ68HawtiPDTJ+0mAS40DXCVa94mJIlaCCHuAFYWWsL9nAn3c6ZXywAASst0HD6Vx+7j2YYEvi8th/ziMrYcO2t0S5i7gzVNA1wNS5MAV1zsZOav20EStRBC3KEsLbSE+jgT6uPM4y3Kk/eRU/mG690JqVnsO5nD2fxi/tmfyT/7Mw371/N0oGmAG1GB+uQd6uMk82xXAUnUQgghDCwttDT0caKhjxM9m+tHIxeVlrHvZA4JqVnsSMkiITWLlLMFHDmVz5FT+fy6/TgAtlZaGtd2pWlgecvb18VWusxvkSRqIYQQ12RjaUFUoBtRgW48F60vO5NXxM7j5Yk7ITWL3MLSK7rMvZ1tLiRtN5oGuNLY30UmGakgOVtCCCEqzMPRhgdCvXkg1BvQT9t59HSeIXHvSMniQEYuGTlFLN2bwdK9GQBoNdDA24moQFeiAtxoFuRK3VqOMsr8GiRRCyGEuGVarYb6Xk7U93IyXO8uKC5lz4kcdqScM7S607IL2Z+ey/70XGZvSQXA2daSpoFuNAt0JSpQ3/KWgWrlJFELIYSoEvbWlrQKdqdVsLuhLD27kITUc+xI0be6d53IIqewlDUHT7Hm4ClDvRAvR6ICXWl2ocs9xOvObXVLohZCCHHb+LjY0snFl06NfAH9/d3703LZnnKOHSnn2J6iH6h2KDOPQ5l5/LxVP1DNycaSpoGuRAW4EhXkRlSAK6721qb8KLeNJGohhBAmY2WhJdJf/1S0Pm3rAHA6r+hCi/sc21POsTM1m9yiUtYeOs3aQ6cN+9b1dDBc524W6EYDbycsamCrWxK1EEIIs1LL0YYHw715MFw/UK20TMeBjFy2X0jeO1KySDqdz9FT+uXi7WEO1hY0CXClsb8rDX0cCfFyor6XY7WfAlQStRBCCLNmeckjUZ+5KwiAs/nFhmvd21POkZCSRX5xGRuOnGHDkfKJSDQaCHS3J8TLiRBvRxp4V78ELolaCCFEtePuYG10e1iZTnEoM5ftyVnsPZmtv8adkcu5ghKSzxSQfKaA5YkZhv2rUwKXRC2EEKLas9BqDI9DvUgpxem8Yg5l5nIoI4+DGbkVSuANvB0JMYMELolaCCFEjaTRaPB0ssHTyYa29WoZyg0J/ELiPphxIZFn5pL1LwlceyGBNw1wZcITUbf1c0iiFkIIcUcxSuD1bzyBHztTYJJbwiRRCyGEENxYAtep2x+XJGohhBDiGi5N4KYgE4cKIYQQZkwStRBCCGHGJFELIYQQZkwStRBCCGHGJFELIYQQZqzGj/rW6XQApKWlmTgSIYQQQu9iTrqYo66lxifqjAz9k2VatWpl4kiEEEIIYxkZGQQGBl6zjkYpZYLbt2+f0tJSduzYgbe3N1rtrfX05+bmEh4ezr59+3BycqqkCGs2OWcVJ+es4uScVZycs4qrzHOm0+nIyMggKioKS8trt5lrfKKuTDk5Obi4uJCdnY2zs/P1dxByzm6CnLOKk3NWcXLOKs5U50wGkwkhhBBmTBK1EEIIYcYkUVeAjY0N7733HjY2pnnea3Uk56zi5JxVnJyzipNzVnGmOmdyjVoIIYQwY9KiFkIIIcyYJGohhBDCjEmiFkIIIcyYJOoKmDx5MnXq1MHW1pbWrVuzZcsWU4dktsaMGUPLli1xcnLCy8uLbt26ceDAAVOHVW188sknaDQahg4daupQzNqJEyd4+umn8fDwwM7OjsjISLZu3WrqsMxWWVkZ7777LsHBwdjZ2VGvXj0++OADZKiSsTVr1tC1a1f8/PzQaDQsWLDAaLtSipEjR+Lr64udnR0xMTEcOnSoyuKRRH2D5s6dy7Bhw3jvvffYvn07TZo0oWPHjmRmZpo6NLO0evVq4uLi2LRpE8uWLaOkpIQOHTqQn59v6tDMXnx8PF9//TWNGzc2dShm7dy5c0RHR2NlZcXixYvZt28fn3/+OW5ubqYOzWyNHTuWKVOm8OWXX5KYmMjYsWP59NNPmTRpkqlDMyv5+fk0adKEyZMnX3X7p59+ysSJE/nqq6/YvHkzDg4OdOzYkcLCwqoJSIkb0qpVKxUXF2dYLysrU35+fmrMmDEmjKr6yMzMVIBavXq1qUMxa7m5uSokJEQtW7ZM3XfffWrIkCGmDslsDR8+XN19992mDqNa6dKli3r++eeNyh577DEVGxtroojMH6Dmz59vWNfpdMrHx0d99tlnhrKsrCxlY2OjZs+eXSUxSIv6BhQXF7Nt2zZiYmIMZVqtlpiYGDZu3GjCyKqP7OxsANzd3U0ciXmLi4ujS5cuRj9r4uoWLlxIixYtePzxx/Hy8iIqKoqpU6eaOiyz1rZtW1asWMHBgwcB2LlzJ+vWraNz584mjqz6SEpKIj093eh31MXFhdatW1dZPqjxs2dVhtOnT1NWVoa3t7dRube3N/v37zdRVNWHTqdj6NChREdH06hRI1OHY7bmzJnD9u3biY+PN3Uo1cLRo0eZMmUKw4YN46233iI+Pp7BgwdjbW1Nnz59TB2eWRoxYgQ5OTmEhoZiYWFBWVkZH330EbGxsaYOrdpIT08HuGo+uLitskmiFlUuLi6OPXv2sG7dOlOHYrZSU1MZMmQIy5Ytw9bW1tThVAs6nY4WLVrw8ccfAxAVFcWePXv46quvJFH/i59//pmZM2cya9YsIiIiSEhIYOjQofj5+ck5M2PS9X0DatWqhYWFhWFu64syMjLw8fExUVTVw8CBA/nzzz9ZuXIl/v7+pg7HbG3bto3MzEyaNWuGpaUllpaWrF69mokTJ2JpaUlZWZmpQzQ7vr6+hIeHG5WFhYWRkpJioojM3+uvv86IESN44okniIyM5JlnnuHVV19lzJgxpg6t2rj4N/925gNJ1DfA2tqa5s2bs2LFCkOZTqdjxYoVtGnTxoSRmS+lFAMHDmT+/Pn8888/BAcHmzoks9a+fXt2795NQkKCYWnRogWxsbEkJCRgYWFh6hDNTnR09BW3/B08eJCgoCATRWT+CgoK0GqN/+xbWFig0+lMFFH1ExwcjI+Pj1E+yMnJYfPmzVWWD6Tr+wYNGzaMPn360KJFC1q1asWECRPIz8/nueeeM3VoZikuLo5Zs2bx+++/4+TkZLh24+Ligp2dnYmjMz9OTk5XXL93cHDAw8NDruv/i1dffZW2bdvy8ccf06tXL7Zs2cI333zDN998Y+rQzFbXrl356KOPCAwMJCIigh07djB+/Hief/55U4dmVvLy8jh8+LBhPSkpiYSEBNzd3QkMDGTo0KF8+OGHhISEEBwczLvvvoufnx/dunWrmoCqZCx5DTVp0iQVGBiorK2tVatWrdSmTZtMHZLZAq66TJs2zdShVRtye9b1/fHHH6pRo0bKxsZGhYaGqm+++cbUIZm1nJwcNWTIEBUYGKhsbW1V3bp11dtvv62KiopMHZpZWbly5VX/fvXp00cppb9F691331Xe3t7KxsZGtW/fXh04cKDK4pHZs4QQQggzJteohRBCCDMmiVoIIYQwY5KohRBCCDMmiVoIIYQwY5KohRBCCDMmiVoIIYQwY5KohRBCCDMmiVoIIYQwY5KohRCVTqPRsGDBAlOHIUSNIIlaiBqmb9++aDSaK5ZOnTqZOjQhxE2QSTmEqIE6derEtGnTjMpsbGxMFI0Q4lZIi1qIGsjGxgYfHx+jxc3NDdB3S0+ZMoXOnTtjZ2dH3bp1+eWXX4z23717Nw888AB2dnZ4eHjQr18/8vLyjOp8//33REREYGNjg6+vLwMHDjTafvr0abp37469vT0hISEsXLjQsO3cuXPExsbi6emJnZ0dISEhV3yxEELoSaIW4g707rvv0qNHD3bu3ElsbCxPPPEEiYmJAOTn59OxY0fc3NyIj49n3rx5LF++3CgRT5kyhbi4OPr168fu3btZuHAh9evXN3qP0aNH06tXL3bt2sVDDz1EbGwsZ8+eNbz/vn37WLx4MYmJiUyZMoVatWrdvhMgRHVSZfNyCSFMok+fPsrCwkI5ODgYLR999JFSSj8F6SuvvGK0T+vWrVX//v2VUkp98803ys3NTeXl5Rm2//XXX0qr1ar09HSllFJ+fn7q7bff/tcYAPXOO+8Y1vPy8hSgFi9erJRSqmvXruq5556rnA8sRA0n16iFqIHatWvHlClTjMrc3d0Nr9u0aWO0rU2bNiQkJACQmJhIkyZNcHBwMGyPjo5Gp9Nx4MABNBoNJ0+epH379teMoXHjxobXDg4OODs7k5mZCUD//v3p0aMH27dvp0OHDnTr1o22bdve1GcVoqaTRC1EDeTg4HBFV3RlsbOzu6F6VlZWRusajQadTgdA586dSU5OZtGiRSxbtoz27dsTFxfHuHHjKj1eIao7uUYtxB1o06ZNV6yHhYUBEBYWxs6dO8nPzzdsX79+PVqtloYNG+Lk5ESdOnVYsWLFLcXg6elJnz59mDFjBhMmTOCbb765peMJUVNJi1qIGqioqIj09HSjMktLS8OArXnz5tGiRQvuvvtuZs6cyZYtW/juu+8AiI2N5b333qNPnz6MGjWKU6dOMWjQIJ555hm8vb0BGDVqFK+88gpeXl507tyZ3Nxc1q9fz6BBg24ovpEjR9K8eXMiIiIoKirizz//NHxREEIYk0QtRA20ZMkSfH19jcoaNmzI/v37Af2I7Dlz5jBgwAB8fX2ZPXs24eHhANjb27N06VKGDBlCy5Ytsbe3p0ePHowfP95wrD59+lBYWMgXX3zBa6+9Rq1atejZs+cNx2dtbc2bb77JsWPHsLOz45577mHOnDmV8MmFqHk0Sill6iCEELePRqNh/vz5dOvWzdShCCFugFyjFkIIIcyYJGohhBDCjMk1aiHuMHK1S4jqRVrUQgghhBmTRC2EEEKYMUnUQgghhBmTRC2EEEKYMUnUQgghhBmTRC2EEEKYMUnUQgghhBmTRC2EEEKYMUnUQgghhBn7P/AZfIrM9fynAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
        "\n",
        "    # Create a second x-axis for tokens seen\n",
        "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "\n",
        "    fig.tight_layout()  # Adjust layout to make room\n",
        "    plt.savefig(\"loss-plot.pdf\")\n",
        "    plt.show()\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## DECODING STRATEGIES TO CONTROL RANDOMNESS"
      ],
      "metadata": {
        "id": "sMW_N3-MWkY2"
      },
      "id": "sMW_N3-MWkY2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(\"cpu\")\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4WXbK_Ndv_1",
        "outputId": "2240c8ea-6285-4abb-debd-4fa152638d43"
      },
      "id": "A4WXbK_Ndv_1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(256, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
        "    max_new_tokens=25,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6lc1y05dx36",
        "outputId": "b6209c43-3680-48b9-989f-88c45999fa12"
      },
      "id": "k6lc1y05dx36",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Every effort moves you?\"\n",
            "\n",
            "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## DECODING STRATEGY 1: TEMPERATURE SCALING"
      ],
      "metadata": {
        "id": "fbBWHsgBd9Ma"
      },
      "id": "fbBWHsgBd9Ma",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "\n",
        "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # New: Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "\n",
        "        # New: Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "        # Same as before: append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "eaFQ5L83eE8H"
      },
      "id": "eaFQ5L83eE8H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
        "    max_new_tokens=15,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
        "    top_k=25,\n",
        "    temperature=1.4\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3LMryiPeTKV",
        "outputId": "9abfc990-b3cf-449e-eaef-7aa5f80fd1dd"
      },
      "id": "E3LMryiPeTKV",
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Every effort moves you know Jack G to a aquite was not! such told all-- enough\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## LOADING AND SAVING MODEL WEIGHTS IN PYTORCH"
      ],
      "metadata": {
        "id": "nO1rcy81eVcJ"
      },
      "id": "nO1rcy81eVcJ",
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "torch.save(model.state_dict(), \"model.pth\")"
      ],
      "metadata": {
        "id": "2gYOS_SRfBTE"
      },
      "id": "2gYOS_SRfBTE",
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.load_state_dict(torch.load(\"model.pth\"))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YgAk7GkfDEE",
        "outputId": "01a44200-67c7-4d0c-9227-ad98677f94f4"
      },
      "id": "-YgAk7GkfDEE",
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-188-f3ec32668201>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"model.pth\"))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(256, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tqdm\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"tqdm version:\", tqdm.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8jti-Z8fOm2",
        "outputId": "4be72797-a713-4b85-9e09-f161cf0d9a37"
      },
      "id": "q8jti-Z8fOm2",
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.17.1\n",
            "tqdm version: 4.66.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## download the gpt_download.py Python module directly from this chapter's online repository\n",
        "## import the download_and_load_gpt2 function from the gpt_download.py file as follows, which will load the GPT-2 architecture settings (settings) and weight parameters (params) into our Python session:"
      ],
      "metadata": {
        "id": "HODT8Eo2fQbE"
      },
      "id": "HODT8Eo2fQbE",
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt_download3 import download_and_load_gpt2"
      ],
      "metadata": {
        "id": "zJezG_7PfaDG"
      },
      "id": "zJezG_7PfaDG",
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hd3gr8ihfbXA",
        "outputId": "ef2de440-580e-49d5-a435-f8922017b189"
      },
      "id": "Hd3gr8ihfbXA",
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 99.5kiB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 5.18MiB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 200kiB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [00:11<00:00, 43.1MiB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 7.28MiB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "model.ckpt.meta: 100%|██████████| 471k/471k [00:00<00:00, 3.36MiB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 3.57MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Settings:\", settings)\n",
        "print(\"Parameter dictionary keys:\", params.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaJSJYo_f2W1",
        "outputId": "9aeb16cd-7ac2-4cb7-ffc6-1e66be6dc92b"
      },
      "id": "LaJSJYo_f2W1",
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
            "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(params[\"wte\"])\n",
        "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLgnDj-Gf8Pm",
        "outputId": "67d13321-e7ca-4c0a-c598-19a5c86a78d3"
      },
      "id": "mLgnDj-Gf8Pm",
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
            "   0.04531523]\n",
            " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
            "   0.04318958]\n",
            " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
            "  -0.08785918]\n",
            " ...\n",
            " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
            "  -0.06952604]\n",
            " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
            "  -0.02245961]\n",
            " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
            "   0.12067825]]\n",
            "Token embedding weight tensor dimensions: (50257, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model configurations in a dictionary for compactness\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "# Copy the base configuration and update with specific model settings\n",
        "model_name = \"gpt2-small (124M)\"  # Example model name\n",
        "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
        "NEW_CONFIG.update(model_configs[model_name])\n"
      ],
      "metadata": {
        "id": "38pHXxaUf9_V"
      },
      "id": "38pHXxaUf9_V",
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
        "gpt = GPTModel(NEW_CONFIG)\n",
        "gpt.eval();"
      ],
      "metadata": {
        "id": "6rObmZzff_ee"
      },
      "id": "6rObmZzff_ee",
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
        "    return torch.nn.Parameter(torch.tensor(right))"
      ],
      "metadata": {
        "id": "k2wUuXthgDTU"
      },
      "id": "k2wUuXthgDTU",
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_weights_into_gpt(gpt, params):\n",
        "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
        "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
        "\n",
        "    for b in range(len(params[\"blocks\"])):\n",
        "        q_w, k_w, v_w = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
        "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
        "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
        "\n",
        "        q_b, k_b, v_b = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
        "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
        "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
        "\n",
        "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.weight,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.bias,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].norm1.scale = assign(\n",
        "            gpt.trf_blocks[b].norm1.scale,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm1.shift = assign(\n",
        "            gpt.trf_blocks[b].norm1.shift,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "        gpt.trf_blocks[b].norm2.scale = assign(\n",
        "            gpt.trf_blocks[b].norm2.scale,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm2.shift = assign(\n",
        "            gpt.trf_blocks[b].norm2.shift,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "\n",
        "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
        "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
        "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "y0ceCS3igEnr"
      },
      "id": "y0ceCS3igEnr",
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_weights_into_gpt(gpt, params)\n",
        "gpt.to(device);"
      ],
      "metadata": {
        "id": "Nm2t0s1MgGNV"
      },
      "id": "Nm2t0s1MgGNV",
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "token_ids = generate(\n",
        "    model=gpt,\n",
        "    idx=text_to_token_ids(\"Every effort moves you to\", tokenizer).to(device),\n",
        "    max_new_tokens=50,\n",
        "    context_size=NEW_CONFIG[\"context_length\"],\n",
        "    top_k=50,\n",
        "    temperature=1\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3BWr71UgKy0",
        "outputId": "72d40f2c-52cf-4820-aa93-b01d756c7eb4"
      },
      "id": "G3BWr71UgKy0",
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Every effort moves you to the edge, to the end, from point A to point B; not only at the end of a journey to the end-point, but also by those who are in their last stages of life.\"\n",
            "\n",
            "The final chapter of the tale is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vHjxsC0YgM0G"
      },
      "id": "vHjxsC0YgM0G",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}